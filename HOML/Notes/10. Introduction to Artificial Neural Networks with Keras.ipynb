{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brain’s architecture was inspiration on how to build an intelligent machine. This is what sparked artificial neural networks (ANNs)\n",
    "\n",
    "- **ANN**: A Machine Learning model inspired by the networks of biological neurons found in our brains.\n",
    "\n",
    "ANNs are at the very core of Deep Learning. \n",
    "\n",
    "They are used for Googles image recognization, apple's siri, recomendation systems, or learning games. \n",
    "\n",
    "# From Biological to Artificial Neurons\n",
    "\n",
    "- **Connectionism** :the study of neural network\n",
    "\n",
    "- ANNs frequently outperform other ML techniques on very large and complex problems.\n",
    "\n",
    "- Increases in computing power have made training possible. \n",
    "- Anns rarely get stuck at a local optima\n",
    "\n",
    "# Biological Neurons\n",
    "\n",
    "- **Cell body**: containing the nucleus and most of the cell’s complex components\n",
    "- **Dendrites**: branching extensions\n",
    "- **Axon**: One very long extension called of the cell body\n",
    "- **Telodendria**:  The axon splits off into many branches near its extremity. \n",
    "More bio stuff, don't have to remember? \n",
    "\n",
    "Individual biological neurons seem to behave in a rather simple way, but they are organized in a vast network of billions.\n",
    "\n",
    "# Logical Computations with Neurons\n",
    "\n",
    "- **Artificial neuron**: it has one or more binary (on/off) inputs and one binary output. \n",
    "- essentially a simple if statement\n",
    "\n",
    "Artificial neurons activates its output when more than a certain number of its inputs are active\n",
    "\n",
    "Figure 10-3 Artifical neurons operations \n",
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1003.png)\n",
    "\n",
    "- The first network on the left is the identity function: if neuron A is activated, then neuron C gets activated as well (since it receives two input signals from neuron A); but if neuron A is off, then neuron C is off as well.\n",
    "\n",
    "- The second network performs a logical AND: neuron C is activated only when both neurons A and B are activated (a single input signal is not enough to activate neuron C).\n",
    "\n",
    "- The third network performs a logical OR: neuron C gets activated if either neuron A or neuron B is activated (or both).\n",
    "\n",
    "- Finally, if we suppose that an input connection can inhibit the neuron’s activity (which is the case with biological neurons), then the fourth network computes a slightly more complex logical proposition: neuron C is activated only if neuron A is active and neuron B is off. If neuron A is active all the time, then you get a logical NOT: neuron C is active when neuron B is off, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron\n",
    "\n",
    "- **Perceptron**: a slightly different artificial neuron where the input and output are numbers. And  each input connection is associated with a weight. \n",
    "\n",
    "- **Linear threshold unit**: computes a weighted sum of its inputs  $\n",
    "\\left(z=w_{1} x_{1}+w_{2} x_{2}+\\cdots+w_{n} x_{n}=\\mathbf{x}^{\\top} \\mathbf{w}\\right)\n",
    "$ then applies a step function to that sum and outputs the result $\n",
    "h_{\\mathbf{w}}(\\mathbf{x})=\\operatorname{step}(z), \\text { where } z=\\mathbf{x}^{\\top} \\mathbf{w}\n",
    "$ \n",
    "\n",
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1004.png)\n",
    "\n",
    "The most common step function used in Perceptrons is the Heaviside step function\n",
    "Sometimes the sign function is used instead.\n",
    "\n",
    "Equation 10-1. Common step functions used in Perceptrons (assuming threshold = 0)\n",
    "$$\n",
    "\\text { heaviside }(z)=\\left\\{\\begin{array}{ll}\n",
    "0 & \\text { if } z<0 \\\\\n",
    "1 & \\text { if } z \\geq 0\n",
    "\\end{array} \\quad \\operatorname{sgn}(z)=\\left\\{\\begin{array}{ll}\n",
    "-1 & \\text { if } z<0 \\\\\n",
    "0 & \\text { if } z=0 \\\\\n",
    "+1 & \\text { if } z>0\n",
    "\\end{array}\\right.\\right.\n",
    "$$\n",
    "\n",
    "- A single TLU can be used for simple linear binary classification\n",
    "\n",
    "EX: Ue a single TLU to classify iris flowers based on petal length and width \n",
    "\n",
    "- Training a TLU in this case means finding the right values for $w_0$, $w_1$, and $w_2$\n",
    "\n",
    "- A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs. \n",
    "\n",
    "-  **fully connected layer**, or a **dense layer**: When all the neurons in a layer are connected to every neuron in the previous layer.\n",
    "\n",
    "- **input neurons**: they output whatever input they are fed.\n",
    "\n",
    "- **Input layer**: All of the input neurons\n",
    "\n",
    "- **bias neuron**: Represents a bias feature. outputs 1 all the time\n",
    "\n",
    " A Perceptron with two inputs and three outputs is represented in Figure 10-5 below. This Perceptron can classify instances simultaneously into three different binary classes, which makes it a multilabel classifier.\n",
    " \n",
    " ![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1005.png)\n",
    " \n",
    " \n",
    " We can compute the output of a layer of neurons all at once. \n",
    " \n",
    "Eqn 10-2:  Computing the outputs of a fully connected layer \n",
    " $$\n",
    "h_{\\mathbf{W}, \\mathbf{b}}(\\mathbf{X})=\\phi(\\mathbf{X} \\mathbf{W}+\\mathbf{b})\n",
    "$$\n",
    "\n",
    "- X represents the matrix of input features. It has one row per instance and one column per feature.\n",
    "\n",
    "- The weight matrix W contains all the connection weights except for the ones from the bias neuron. It has one row per input neuron and one column per artificial neuron in the layer.\n",
    "\n",
    "- The bias vector b contains all the connection weights between the bias neuron and the artificial neurons. It has one bias term per artificial neuron.\n",
    "\n",
    "- The function ϕ is called the activation function: when the artificial neurons are TLUs, it is a step function.\n",
    "\n",
    "-  **Hebb’s rule** (or Hebbian learning):the connection weight between two neurons tends to increase when they fire simultaneously. \n",
    "\n",
    "This Perceptron learning rule reinforces connections that help reduce the error.\n",
    "\n",
    "More specifically, the Perceptron is fed one training instance at a time, and for each instance it makes its predictions. For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction. The rule is shown in Equation 10-3 below.\n",
    "\n",
    "$$\n",
    "w_{i, j}^{(\\text {next step })}=w_{i, j}+\\eta\\left(y_{j}-\\hat{y}_{j}\\right) x_{i}\n",
    "$$\n",
    "\n",
    "- $w_{i, j}$ is the connection weight between the ith input neuron and the jth output neuron.\n",
    "\n",
    "- $x_i$ is the ith input value of the current training instance.\n",
    "\n",
    "- $\\hat{y}_j$ is the output of the jth output neuron for the current training instance.\n",
    "\n",
    "- $y_j$ is the target output of the jth output neuron for the current training instance.\n",
    "\n",
    "- η is the learning rate.\n",
    "\n",
    "\n",
    "The decision boundary of each output neuron is linear, so Perceptrons are incapable of learning complex patterns \n",
    "\n",
    "- **Perceptron convergence theorem.**  If the training instances are linearly separable,this algorithm would converge to a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int)  # Iris setosa?\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perceptron learning algorithm strongly resembles Stochastic Gradient Descent.\n",
    "\n",
    "Scikit-Learn’s Perceptron class is equivalent to using an SGDClassifier with the following hyperparameters: loss=\"perceptron\", learning_rate=\"constant\", eta0=1 (the learning rate), and penalty=None (no regularization).\n",
    "\n",
    "- Perceptrons do not output a class probability. This is one reason to prefer Logistic Regression over Perceptrons.\n",
    "\n",
    "limitations of Perceptrons can be eliminated by stacking multiple Perceptrons.\n",
    "\n",
    "- **Multilayer Perceptron**: Multiple Stacked Perceptrons \n",
    "\n",
    "# The Multilayer Perceptron and Backpropagation\n",
    "\n",
    "- An MLP is composed of one input layer, one or more layers of TLUs(hidden layers)  and one final layer of TLUs called the output layer. \n",
    "\n",
    "- **lower layers**: layers close to the input layer\n",
    "\n",
    "Every layer except the output layer includes a bias neuron and is fully connected to the next layer.\n",
    "\n",
    "- **deep neural network (DNN)**: When an ANN contains a deep stack of hidden layers\n",
    "\n",
    "\n",
    "The trainging algorithms for MLP is \n",
    "\n",
    "- **backpropagation training algorithm**: In short, it is Gradient Descent using an efficient technique for computing the gradients automatically. Determines how to tweak the weights to reduce error. \n",
    "\n",
    "- **NOTE**:  Automatically computing gradients is called automatic differentiation, or autodiff. There are various autodiff techniques, with different pros and cons. The one used by backpropagation is called reverse-mode autodiff. It is fast and precise, and is well suited when the function to differentiate has many variables (e.g., connection weights) and few outputs (e.g., one loss). If you want to learn more about autodiff, check out Appendix D in the book.\n",
    "\n",
    "Let’s run through this algorithm in a bit more detail:\n",
    "\n",
    "- It handles one mini-batch at a time (for example, containing 32 instances each), and it goes through the full training set multiple times. Each pass is called an epoch.\n",
    "\n",
    "- Each mini-batch is passed to the network’s input layer, which sends it to the first hidden layer. The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch). The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the output layer. This is the forward pass: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
    "\n",
    "- Next, the algorithm measures the network’s output error (i.e., it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).\n",
    "\n",
    "- Then it computes how much each output connection contributed to the error. This is done analytically by applying the chain rule (perhaps the most fundamental rule in calculus), which makes this step fast and precise.\n",
    "\n",
    "- The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule, working backward until the algorithm reaches the input layer. As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network (hence the name of the algorithm).\n",
    "\n",
    "- Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.\n",
    "\n",
    "Summarzing this : for each training instance, the backpropagation algorithm first makes a prediction (forward pass) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally tweaks the connection weights to reduce the error (Gradient Descent step).\n",
    "\n",
    "- **WARNING** : It is important to initialize all the hidden layers’ connection weights randomly, or else training will fail. For example, if you initialize all weights and biases to zero, then all neurons in a given layer will be perfectly identical, and thus backpropagation will affect them in exactly the same way, so they will remain identical. In other words, despite having hundreds of neurons per layer, your model will act as if it had only one neuron per layer: it won’t be too smart. If instead you randomly initialize the weights, you break the symmetry and allow backpropagation to train a diverse team of neurons.\n",
    "\n",
    "- In order for this algorithm to work properly the step function is the logistic (sigmoid) function $\\sigma(z)=1 /(1+\\exp (-z))$. This adds a hill(a gradient can then be used )\n",
    "\n",
    "\n",
    "### Other Popular Step Functions \n",
    "\n",
    "The hyperbolic tangent function: tanh(z) = 2σ(2z) – 1\n",
    "\n",
    "    Just like the logistic function, this activation function is S-shaped, continuous, and differentiable, but its output value ranges from –1 to 1 (instead of 0 to 1 in the case of the logistic function). That range tends to make each layer’s output more or less centered around 0 at the beginning of training, which often helps speed up convergence.\n",
    "    \n",
    "The Rectified Linear Unit function: ReLU(z) = max(0, z)\n",
    "\n",
    "     The ReLU function is continuous but unfortunately not differentiable at z = 0 (the slope changes abruptly, which can make Gradient Descent bounce around), and its derivative is 0 for z < 0. In practice, however, it works very well and has the advantage of being fast to compute, so it has become the default.13 Most importantly, the fact that it does not have a maximum output value helps reduce some issues during Gradient Descent).\n",
    "     \n",
    "- **activation functions**: A step function \n",
    "\n",
    "If we don't use activation functions then each layer will be considered a layer. This is because they all solve a problem with the same complexity. \n",
    "\n",
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1008.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression MLPs\n",
    "\n",
    "- MLPs can be used for regression tasks\n",
    "\n",
    "-  If you want to predict a single value you just need a single output neuron.\n",
    "\n",
    "- For multivariate regression, you need one output neuron per output dimension\n",
    "\n",
    "- For MLP for regression, you do not want to use any activation function for the output neurons\n",
    "\n",
    "-  To guarantee that the output will always be positive, then you can use the ReLU activation function in the output layer.  \n",
    "\n",
    "- **Softplus activation function**: A smooth variant of ReLU, softplus(z) = log(1 + exp(z))\n",
    "\n",
    "- If you want to guarantee that the predictions will fall within a given range of values, then you can use the logistic function or the hyperbolic tangent, then scale the labels to the appropriate range. \n",
    "\n",
    "Typical Loss function \n",
    "\n",
    "- mean squared error\n",
    "\n",
    "- if you have a lot of outliers in the training set, you may prefer to use the mean absolute error instead\n",
    "\n",
    "Huber loss, which is a combination of both.\n",
    "\n",
    "**TIP**: The Huber loss is quadratic when the error is smaller than a threshold δ (typically 1) but linear when the error is larger than δ. The linear part makes it less sensitive to outliers than the mean squared error, and the quadratic part allows it to converge faster and be more precise than the mean absolute error.\n",
    "\n",
    "\n",
    "### Typical  architecture of a regression MLP\n",
    "\n",
    "| Hyperparameter\t|Typical value |\n",
    "|-------------------|--------------|\n",
    "| # input neurons   | One per input feature (e.g., 28 x 28 = 784 for MNIST) | \n",
    "| # hidden layers   | Depends on the problem, but typically 1 to 5  |\n",
    "|# neurons per hidden layer| Depends on the problem, but typically 10 to 100|\n",
    "|# output neurons    | 1 per prediction dimension |\n",
    "| Hidden activation  | ReLU (or SELU, see Chapter 11) | \n",
    "| Output activation  | None, or ReLU/softplus (if positive outputs) or logistic/tanh (if bounded outputs) |\n",
    "| Loss function    | MSE or MAE/Huber (if outliers) | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification MLPs\n",
    "\n",
    "- For a binary classification problem, you just need a single output neuron using the logistic activation function\n",
    "\n",
    "- the output will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class\n",
    "\n",
    "- The estimated probability of the negative class is equal to one minus that number.\n",
    "\n",
    "Multilabel binary classification \n",
    "\n",
    "- Dedicate one output neuron for each positive class\n",
    "\n",
    "- If each instance can belong only to a single class (eg a single digit from 0 thru 9) you need to have one output neuron per class,.\n",
    "\n",
    "- Use a softmax activation function for the whole output layer. \n",
    "\n",
    "- **softmax function**: ensures that all the estimated probabilities are between 0 and 1 and that they add up to 1 \n",
    "\n",
    "Loss Function \n",
    "\n",
    "- Cross-entropy loss (also called the log loss) is generally a good choice. \n",
    "\n",
    "\n",
    "Typical architecture of a classification MLP.\n",
    "\n",
    "|Hyperparameter\t         | Binary classification\t|Multilabel binary classification | Multiclass classification |\n",
    "|------------------------|--------------------------|---------------------------------|---------------------------|\n",
    "|Input and hidden layers | Same as regression       |Same as regression               | Same as regression        |\n",
    "| # output neurons       | 1                        |1 per label                      |       1 per class         |\n",
    "|Output layer activation |Logistic                  | Logistic                        | Softmax                   |\n",
    "| Loss function          |Cross entropy             |Cross entropy                    |Cross entropy              |\n",
    "\n",
    "\n",
    "You have all the concepts you need to start implementing MLPs with Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing MLPs with Keras\n",
    "\n",
    "\n",
    "**Multibackend Keras**\n",
    "\n",
    "-  To perform the heavy computations required by neural networks it  relies on a computation backend. \n",
    "\n",
    "- you can choose from three popular open source Deep Learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK), and Theano. other implementations have been released JavaScript or TypeScript (to run Keras code in a web browser), and PlaidML (which can run on all sorts of GPU devices, not just Nvidia) and many more. \n",
    "\n",
    "- TensorFlow itself now comes bundled with its own Keras implementation, tf.keras. This allows us to use Tensorflow Apis' such as TF Data API.  \n",
    "\n",
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1010.png)\n",
    "\n",
    "- The most popular Deep Learning library, after Keras and TensorFlow, is Facebook’s PyTorch library.\n",
    "\n",
    "- Once you know Keras, it is not difficult to switch to PyTorch. They were inspired by sklearn and chainer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__\n",
    "#the version of the Keras API implemented by tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Image Classifier Using the Sequential API\n",
    "\n",
    "-  We will use Fashion MNIST\n",
    "\n",
    "- 70,000 grayscale images of 28 × 28 pixels each, with 10 classes\n",
    "\n",
    "- the images represent fashion items rather than handwritten digits\n",
    "\n",
    "- Thus the problem turns out to be significantly more challenging than MNIST.\n",
    "\n",
    "- A simple linear model reaches about 92% accuracy on MNIST, but only about 83% on Fashion MNIST.\n",
    "\n",
    "## USING KERAS TO LOAD THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 5us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 6s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Keras provides some utility functions to fetch and load common datasets\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t every image is represented as a 28 × 28 array rather than a 1D array of size 784\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no validation set, so we’ll create one now. since we are going to train the neural network using Gradient Descent, we must scale the input features. For simplicity, we’ll scale the pixel intensities down to the 0–1 range by dividing them by 255.0 .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Fashion MNIST we need the list of class names to know what we are dealing with:\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "# the first image in the training set represents a coat:\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples from the Fashion MSNT Dataset \n",
    "\n",
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1011.png)\n",
    "\n",
    "# CREATING THE MODEL USING THE SEQUENTIAL API\n",
    "\n",
    "Here is a classification MLP with two hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go through this code line by line:\n",
    "\n",
    "- The first line creates a Sequential model. This is the simplest kind of Keras model for neural networks that are just composed of a single stack of layers connected sequentially. This is called the Sequential API.\n",
    "\n",
    "- Next, we build the first layer and add it to the model. It is a Flatten layer whose role is to convert each input image into a 1D array: if it receives input data X, it computes X.reshape(-1, 28*28). This layer does not have any parameters; it is just there to do some simple preprocessing. Since it is the first layer in the model, you should specify the input_shape, which doesn’t include the batch size, only the shape of the instances. Alternatively, you could add a keras.layers.InputLayer as the first layer, setting input_shape=[28,28].\n",
    "\n",
    "- Next we add a Dense hidden layer with 300 neurons. It will use the ReLU activation function. Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes Equation 10-2.\n",
    "\n",
    "- Then we add a second Dense hidden layer with 100 neurons, also using the ReLU activation function.\n",
    "\n",
    "- Finally, we add a Dense output layer with 10 neurons (one per class), using the softmax activation function (because the classes are exclusive).\n",
    "\n",
    "\n",
    "- **TIP**: Specifying activation=\"relu\" is equivalent to specifying activation=keras.activations.relu. Other activation functions are available in the keras.activations package, we will use many of them in this book. See https://keras.io/activations/ for the full list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of adding the layers one by one as we just did, \n",
    "# you can pass a list of layers when creating the Sequential model:\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING CODE EXAMPLES FROM KERAS.IO\n",
    "\n",
    "Code examples documented on keras.io will work fine with tf.keras, but you need to change the imports. For example, consider this keras.io code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "# output_layer = Dense(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You must change the imports like this:\n",
    "from tensorflow.keras.layers import Dense\n",
    "output_layer = Dense(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is more verbose, but I use it in this book so you can easily see which packages to use, and to avoid confusion between standard classes and custom classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model’s summary() method displays all the model’s layers \n",
    "\n",
    "The summary includes:\n",
    "\n",
    "- Each layer’s name\n",
    "- its output shape\n",
    "- number of parameters\n",
    "\n",
    "- The summary ends with the total number of parameters, including trainable and non-trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dense layers** often have a lot of parameters. \n",
    "\n",
    "- For example, the first hidden layer has 784 × 300 connection weights, plus 300 bias terms, which adds up to 235,500 parameters\n",
    "\n",
    "- This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting especially when you do not have a lot of training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x1ef67c7cf60>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1ef70634ef0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1ef7645aa90>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1ef76464630>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can easily get a model’s list of layers, to fetch a layer by its index, or you can fetch it by name:\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense_3') is hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.053266  ,  0.01534975, -0.01698087, ..., -0.05883894,\n",
       "        -0.0068607 ,  0.06915954],\n",
       "       [-0.03469538,  0.06661914, -0.00159548, ...,  0.03484789,\n",
       "        -0.06824687,  0.00852676],\n",
       "       [ 0.05458854,  0.05302885,  0.02688093, ...,  0.02450662,\n",
       "        -0.00050273,  0.00813788],\n",
       "       ...,\n",
       "       [ 0.03552338, -0.06078802,  0.06078289, ...,  0.00366828,\n",
       "         0.05655251, -0.03215222],\n",
       "       [-0.05883799, -0.0716325 , -0.05388476, ...,  0.0407314 ,\n",
       "        -0.03563473,  0.0642748 ],\n",
       "       [ 0.06628798,  0.01696514, -0.05609043, ..., -0.00144854,\n",
       "         0.03427551, -0.04280059]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access parameters with get_weights() and set_weights()\n",
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dense layer initialized the connection weights randomly( is needed to break symmetry) and  biases were initialized to zeros, which is fine.\n",
    "\n",
    "- For a different initialization method, you can set kernel_initializer (kernel is another name for the matrix of connection weights) or bias_initializer when creating the layer.\n",
    "\n",
    "-  initialization method will be further dicussed in chapter 11\n",
    "\n",
    "\n",
    "- NOTE:  The shape of the weight matrix depends on the number of inputs. This is why it is recommended to specify the input_shape when creating the first layer in a Sequential model. However, if you do not specify the input shape, it’s OK: Keras will simply wait until it knows the input shape before it actually builds the model. This will happen either when you feed it actual data (e.g., during training), or when you call its build() method. Until the model is really built, the layers will not have any weights, and you will not be able to do certain things (such as print the model summary or save the model). So, if you know the input shape when creating the model, it is best to specify it.\n",
    "\n",
    "\n",
    "# COMPILING THE MODEL\n",
    "\n",
    "After a model is created, you must call its compile() method to specify the loss function and the optimizer to use. Optionally, you can specify a list of extra metrics to compute during training and evaluation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Using loss=\"sparse_categorical_crossentropy\" is equivalent to using loss=keras.losses.sparse_categorical_crossentropy. Similarly, specifying optimizer=\"sgd\" is equivalent to specifying optimizer=keras.optimizers.SGD(), and metrics=[\"accuracy\"] is equivalent to metrics=[keras.metrics.sparse_categorical_accuracy] (when using this loss). We will use many other losses, optimizers, and metrics in this book; for the full lists,\n",
    "\n",
    "\n",
    "- We use the \"sparse_categorical_crossentropy\" loss because we have sparse labels (i.e., for each instance, there is just a target class index, from 0 to 9 in this case), and the classes are exclusive.\n",
    "\n",
    "\n",
    "- If instead we had one target probability per class for each instance (such as one-hot vectors to represent class 3), then we would need to use the \"categorical_crossentropy\" loss instead.\n",
    "\n",
    "\n",
    "- were doing binary classification or multilabel binary classification, then we would use the \"sigmoid\"  activation function in the output layer instead of the \"softmax\" activation function, and we would use the \"binary_crossentropy\" loss.\n",
    "\n",
    "\n",
    "- **TIP**: If you want to convert sparse labels (i.e., class indices) to one-hot vector labels, use the keras.utils.to_categorical() function. To go the other way round, use the np.argmax() function with axis=1.\n",
    "\n",
    "Regarding the optimizer, \"sgd\" means that we will train the model using simple Stochastic Gradient Descent. In other words, Keras will perform the backpropagation algorithm described earlier (i.e., reverse-mode autodiff plus Gradient Descent). We will discuss more efficient optimizers in Chapter 11 (they improve the Gradient Descent part, not the autodiff).\n",
    "\n",
    "\n",
    "\n",
    "- NOTE: When using the SGD optimizer, it is important to tune the learning rate. So, you will generally want to use optimizer=keras.optimizers.SGD(lr=???) to set the learning rate, rather than optimizer=\"sgd\", which defaults to lr=0.01.\n",
    "\n",
    " # TRAINING AND EVALUATING THE MODEL\n",
    " \n",
    "Now the model is ready to be trained. For this we simply need to call its fit() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.7139 - accuracy: 0.7655 - val_loss: 0.5096 - val_accuracy: 0.8316\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4854 - accuracy: 0.8300 - val_loss: 0.4447 - val_accuracy: 0.8500\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4426 - accuracy: 0.8449 - val_loss: 0.4794 - val_accuracy: 0.8410\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4163 - accuracy: 0.8532 - val_loss: 0.3975 - val_accuracy: 0.8636\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3957 - accuracy: 0.8602 - val_loss: 0.3956 - val_accuracy: 0.8628\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3800 - accuracy: 0.8647 - val_loss: 0.3700 - val_accuracy: 0.8718\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3673 - accuracy: 0.8695 - val_loss: 0.3612 - val_accuracy: 0.8730\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3547 - accuracy: 0.8735 - val_loss: 0.3530 - val_accuracy: 0.8760\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3445 - accuracy: 0.8761 - val_loss: 0.3612 - val_accuracy: 0.8698\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3354 - accuracy: 0.8793 - val_loss: 0.3468 - val_accuracy: 0.8768\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3258 - accuracy: 0.8831 - val_loss: 0.3349 - val_accuracy: 0.8800\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3177 - accuracy: 0.8855 - val_loss: 0.3257 - val_accuracy: 0.8818\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3105 - accuracy: 0.8883 - val_loss: 0.3211 - val_accuracy: 0.8880\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3046 - accuracy: 0.8902 - val_loss: 0.3672 - val_accuracy: 0.8662\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2976 - accuracy: 0.8932 - val_loss: 0.3277 - val_accuracy: 0.8840\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2915 - accuracy: 0.8949 - val_loss: 0.3155 - val_accuracy: 0.8872\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2853 - accuracy: 0.8961 - val_loss: 0.3392 - val_accuracy: 0.8758\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2791 - accuracy: 0.8991 - val_loss: 0.3100 - val_accuracy: 0.8872\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2737 - accuracy: 0.9014 - val_loss: 0.3081 - val_accuracy: 0.8896\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2688 - accuracy: 0.9033 - val_loss: 0.3251 - val_accuracy: 0.8878\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2636 - accuracy: 0.9053 - val_loss: 0.3115 - val_accuracy: 0.8878\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2590 - accuracy: 0.9058 - val_loss: 0.3161 - val_accuracy: 0.8858\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2547 - accuracy: 0.9076 - val_loss: 0.3030 - val_accuracy: 0.8940\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2498 - accuracy: 0.9094 - val_loss: 0.2927 - val_accuracy: 0.89265 - accuracy: 0.90\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2450 - accuracy: 0.9121 - val_loss: 0.2956 - val_accuracy: 0.8920\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2413 - accuracy: 0.9129 - val_loss: 0.2989 - val_accuracy: 0.8934\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2383 - accuracy: 0.9139 - val_loss: 0.3099 - val_accuracy: 0.8834\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2331 - accuracy: 0.9157 - val_loss: 0.2933 - val_accuracy: 0.8950\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2301 - accuracy: 0.9166 - val_loss: 0.2985 - val_accuracy: 0.8924\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2259 - accuracy: 0.9199 - val_loss: 0.2901 - val_accuracy: 0.8974\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\JungleBook\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\JungleBook\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./models\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./models')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
