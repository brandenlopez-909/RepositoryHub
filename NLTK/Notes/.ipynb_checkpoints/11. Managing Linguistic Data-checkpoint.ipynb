{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured collections of annotated linguistic data are essential in most areas of NLP, however, we still face many obstacles in using them. The goal of this chapter is to answer the following questions:\n",
    "\n",
    "- How do we design a new language resource and ensure that its coverage, balance, and documentation support a wide range of uses?\n",
    "- When existing data is in the wrong format for some analysis tool, how can we convert it to a suitable format?\n",
    "- What is a good way to document the existence of a resource we have created so that others can easily find it?\n",
    "\n",
    "\n",
    "Along the way, we will study the design of existing corpora, the typical workflow for creating a corpus, and the lifecycle of corpus.\n",
    "\n",
    "# 1   Corpus Structure: a Case Study\n",
    "\n",
    "The TIMIT corpus of read speech was the first annotated speech database to be widely distributed, and it has an especially clear organization. TIMIT was developed by a consortium including Texas Instruments and MIT, from which it derives its name. It was designed to provide data for the acquisition of acoustic-phonetic knowledge and to support the development and evaluation of automatic speech recognition systems.\n",
    "\n",
    "# 1.1   The Structure of TIMIT\n",
    "Like the Brown Corpus, which displays a balanced selection of text genres and sources, TIMIT includes a balanced selection of dialects, speakers, and materials. For each of eight dialect regions, 50 male and female speakers having a range of ages and educational backgrounds each read ten carefully chosen sentences. Two sentences, read by all speakers, were designed to bring out dialect variation:\n",
    "\n",
    "(1)\t\t\n",
    "\n",
    "    a.\t\tshe had your dark suit in greasy wash water all year\n",
    "\n",
    "    b.\t\tdon't ask me to carry an oily rag like that\n",
    "\n",
    "The remaining sentences were chosen to be phonetically rich, involving all phones (sounds) and a comprehensive range of diphones (phone bigrams). Additionally, the design strikes a balance between multiple speakers saying the same sentence in order to permit comparison across speakers, and having a large range of sentences covered by the corpus to get maximal coverage of diphones. Five of the sentences read by each speaker are also read by six other speakers (for comparability). The remaining three sentences read by each speaker were unique to that speaker (for coverage).\n",
    "\n",
    "NLTK includes a sample from the TIMIT corpus. You can access its documentation in the usual way, using help(nltk.corpus.timit). Print nltk.corpus.timit.fileids() to see a list of the 160 recorded utterances in the corpus sample. Each file name has internal structure as shown in 1.1.\n",
    "\n",
    "![](https://www.nltk.org/images/timit.png)\n",
    "\n",
    "Each item has a phonetic transcription which can be accessed using the phones() method. We can access the corresponding word tokens in the customary way. Both access methods permit an optional argument offset=True which includes the start and end offsets of the corresponding span in the audio file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h#',\n",
       " 'sh',\n",
       " 'iy',\n",
       " 'hv',\n",
       " 'ae',\n",
       " 'dcl',\n",
       " 'y',\n",
       " 'ix',\n",
       " 'dcl',\n",
       " 'd',\n",
       " 'aa',\n",
       " 'kcl',\n",
       " 's',\n",
       " 'ux',\n",
       " 'tcl',\n",
       " 'en',\n",
       " 'gcl',\n",
       " 'g',\n",
       " 'r',\n",
       " 'iy',\n",
       " 's',\n",
       " 'iy',\n",
       " 'w',\n",
       " 'aa',\n",
       " 'sh',\n",
       " 'epi',\n",
       " 'w',\n",
       " 'aa',\n",
       " 'dx',\n",
       " 'ax',\n",
       " 'q',\n",
       " 'ao',\n",
       " 'l',\n",
       " 'y',\n",
       " 'ih',\n",
       " 'ax',\n",
       " 'h#']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "phonetic = nltk.corpus.timit.phones('dr1-fvmh0/sa1')\n",
    "phonetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g', 'r', 'iy1', 's', 'iy', 'w', 'ao1', 'sh', 'w', 'ao1', 't', 'axr']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In addition to this text data, TIMIT includes a lexicon that provides the canonical pronunciation of every word, \n",
    "# which can be compared with a particular utterance:\n",
    "timitdict = nltk.corpus.timit.transcription_dict()\n",
    "timitdict['greasy'] + timitdict['wash'] + timitdict['water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g', 'r', 'iy', 's', 'iy', 'w', 'aa', 'sh', 'epi', 'w', 'aa', 'dx', 'ax']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonetic[17:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a sense of what a speech processing system would have to do in producing or recognizing speech in this particular dialect (New England). Finally, TIMIT includes demographic data about the speakers, permitting fine-grained study of vocal, social, and gender characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeakerInfo(id='VMH0', sex='F', dr='1', use='TRN', recdate='03/11/86', birthdate='01/08/60', ht='5\\'05\"', race='WHT', edu='BS', comments='BEST NEW ENGLAND ACCENT SO FAR')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.timit.spkrinfo('dr1-fvmh0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2   Notable Design Features\n",
    "\n",
    "TIMIT illustrates several key features of corpus design. First, the corpus contains two layers of annotation, at the phonetic and orthographic levels. In general, a text or speech corpus may be annotated at many different linguistic levels, including morphological, syntactic, and discourse levels. Moreover, even at a given level there may be different labeling schemes or even disagreement amongst annotators, such that we want to represent multiple versions. \n",
    "\n",
    "A second property of TIMIT is its balance across multiple dimensions of variation, for coverage of dialect regions and diphones. The inclusion of speaker demographics brings in many more independent variables, that may help to account for variation in the data, and which facilitate later uses of the corpus for purposes that were not envisaged when the corpus was created, such as sociolinguistics.\n",
    "\n",
    "A third property is that there is a sharp division between the original linguistic event captured as an audio recording, and the annotations of that event. The same holds true of text corpora, in the sense that the original text usually has an external source, and is considered to be an immutable artifact. Any transformations of that artifact which involve human judgment — even something as simple as tokenization — are subject to later revision, thus it is important to retain the source material in a form that is as close to the original as possible.\n",
    "\n",
    "![](https://www.nltk.org/images/timit-structure.png)\n",
    "\n",
    "A fourth feature of TIMIT is the hierarchical structure of the corpus. With 4 files per sentence, and 10 sentences for each of 500 speakers, there are 20,000 files. These are organized into a tree structure, shown schematically in 1.2 above. At the top level there is a split between training and testing sets, which gives away its intended use for developing and evaluating statistical models.\n",
    "\n",
    "Finally, notice that even though TIMIT is a speech corpus, its transcriptions and associated data are just text, and can be processed using programs just like any other text corpus. Therefore, many of the computational methods described in this book are applicable. Moreover, notice that all of the data types included in the TIMIT corpus fall into the two basic categories of lexicon and text, which we will discuss below. Even the speaker demographics data is just another instance of the lexicon data type.\n",
    "\n",
    "This last observation is less surprising when we consider that text and record structures are the primary domains for the two subfields of computer science that focus on data management, namely text retrieval and databases. A notable feature of linguistic data management is that usually brings both data types together, and that it can draw on results and techniques from both fields.\n",
    "\n",
    "\n",
    "# 1.3   Fundamental Data Types\n",
    "\n",
    "![](https://www.nltk.org/images/datatypes.png)\n",
    "\n",
    "Despite its complexity, the TIMIT corpus only contains two fundamental data types, namely lexicons and texts. As we saw in chapter 2., most lexical resources can be represented using a record structure, i.e. a key plus one or more fields, as shown in figure 1.3 above. A lexical resource could be a conventional dictionary or comparative wordlist, as illustrated. It could also be a phrasal lexicon, where the key field is a phrase rather than a single word. A thesaurus also consists of record-structured data, where we look up entries via non-key fields that correspond to topics. We can also construct special tabulations (known as paradigms) to illustrate contrasts and systematic variation, as shown in 1.3 for three verbs. TIMIT's speaker table is also a kind of lexicon.\n",
    "\n",
    "At the most abstract level, a text is a representation of a real or fictional speech event, and the time-course of that event carries over into the text itself. A text could be a small unit, such as a word or sentence, or a complete narrative or dialogue. It may come with annotations such as part-of-speech tags, morphological analysis, discourse structure, and so forth. As we saw in the IOB tagging technique (7.), it is possible to represent higher-level constituents using tags on individual words. Thus the abstraction of text shown in 1.3 is sufficient.\n",
    "\n",
    "Despite the complexities and idiosyncrasies of individual corpora, at base they are collections of texts together with record-structured data. The contents of a corpus are often biased towards one or other of these types. For example, the Brown Corpus contains 500 text files, but we still use a table to relate the files to 15 different genres. At the other end of the spectrum, WordNet contains 117,659 synset records, yet it incorporates many example sentences (mini-texts) to illustrate word usages. TIMIT is an interesting mid-point on this spectrum, containing substantial free-standing material of both the text and lexicon types.\n",
    "\n",
    "# 2   The Life-Cycle of a Corpus\n",
    "\n",
    "Corpora are not born fully-formed, but involve careful preparation and input from many people over an extended period. Raw data needs to be collected, cleaned up, documented, and stored in a systematic structure. Various layers of annotation might be applied, some requiring specialized knowledge of the morphology or syntax of the language. Success at this stage depends on creating an efficient workflow involving appropriate tools and format converters. Quality control procedures can be put in place to find inconsistencies in the annotations, and to ensure the highest possible level of inter-annotator agreement. Because of the scale and complexity of the task, large corpora may take years to prepare, and involve tens or hundreds of person-years of effort. In this section we briefly review the various stages in the life-cycle of a corpus.\n",
    "\n",
    "# 2.1   Three Corpus Creation Scenarios\n",
    "\n",
    "In one type of corpus, the design unfolds over in the course of the creator's explorations. This is the pattern typical of traditional \"field linguistics,\" in which material from elicitation sessions is analyzed as it is gathered, with tomorrow's elicitation often based on questions that arise in analyzing today's. The resulting corpus is then used during subsequent years of research, and may serve as an archival resource indefinitely. Computerization is an obvious boon to work of this type, as exemplified by the popular program Shoebox, now over two decades old and re-released as Toolbox. Other software tools, even simple word processors and spreadsheets, are routinely used to acquire the data. In the next section we will look at how to extract data from these sources.\n",
    "\n",
    "Another corpus creation scenario is typical of experimental research where a body of carefully-designed material is collected from a range of human subjects, then analyzed to evaluate a hypothesis or develop a technology. It has become common for such databases to be shared and re-used within a laboratory or company, and often to be published more widely. Corpora of this type are the basis of the \"common task\" method of research management, which over the past two decades has become the norm in government-funded research programs in language technology. We have already encountered many such corpora in the earlier chapters; we will see how to write Python programs to implement the kinds of curation tasks that are necessary before such corpora are published.\n",
    "\n",
    "Finally, there are efforts to gather a \"reference corpus\" for a particular language, such as the American National Corpus (ANC) and the British National Corpus (BNC). Here the goal has been to produce a comprehensive record of the many forms, styles and uses of a language. Apart from the sheer challenge of scale, there is a heavy reliance on automatic annotation tools together with post-editing to fix any errors. However, we can write programs to locate and repair the errors, and also to analyze the corpus for balance.\n",
    "\n",
    "# 2.2   Quality Control\n",
    "\n",
    "Good tools for automatic and manual preparation of data are essential. However the creation of a high-quality corpus depends just as much on such mundane things as documentation, training, and workflow. Annotation guidelines define the task and document the markup conventions. They may be regularly updated to cover difficult cases, along with new rules that are devised to achieve more consistent annotations. Annotators need to be trained in the procedures, including methods for resolving cases not covered in the guidelines. A workflow needs to be established, possibly with supporting software, to keep track of which files have been initialized, annotated, validated, manually checked, and so on. There may be multiple layers of annotation, provided by different specialists. Cases of uncertainty or disagreement may require adjudication.\n",
    "\n",
    "Large annotation tasks require multiple annotators, which raises the problem of achieving consistency. How consistently can a group of annotators perform? We can easily measure consistency by having a portion of the source material independently annotated by two people. This may reveal shortcomings in the guidelines or differing abilities with the annotation task. In cases where quality is paramount, the entire corpus can be annotated twice, and any inconsistencies adjudicated by an expert.\n",
    "\n",
    "It is considered best practice to report the inter-annotator agreement that was achieved for a corpus (e.g. by double-annotating 10% of the corpus). This score serves as a helpful upper bound on the expected performance of any automatic system that is trained on this corpus.\n",
    "\n",
    "The Kappa coefficient K measures agreement between two people making category judgments, correcting for expected chance agreement. For example, suppose an item is to be annotated, and four coding options are equally likely. Then two people coding randomly would be expected to agree 25% of the time. Thus, an agreement of 25% will be assigned K = 0, and better levels of agreement will be scaled accordingly. For an agreement of 50%, we would get K = 0.333, as 50 is a third of the way from 25 to 100. Many other agreement measures exist; see help(nltk.metrics.agreement) for details.\n",
    "\n",
    "![](https://www.nltk.org/images/windowdiff.png)\n",
    "\n",
    "We can also measure the agreement between two independent segmentations of language input, e.g. for tokenization, sentence segmentation, named-entity detection. In 2.1 we see three possible segmentations of a sequence of items which might have been produced by annotators (or programs). Although none of them agree exactly, S1 and S2 are in close agreement, and we would like a suitable measure. Windowdiff is a simple algorithm for evaluating the agreement of two segmentations by running a sliding window over the data and awarding partial credit for near misses. If we preprocess our tokens into a sequence of zeros and ones, to record when a token is followed by a boundary, we can represent the segmentations as strings, and apply the windowdiff scorer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"00000010000000001000000\"\n",
    "s2 = \"00000001000000010000000\"\n",
    "s3 = \"00010000000000000001000\"\n",
    "nltk.windowdiff(s1, s1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.windowdiff(s2, s3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the window had a size of 3. The windowdiff computation slides this window across a pair of strings. At each position it totals up the number of boundaries found inside this window, for both strings, then computes the difference. These differences are then summed. We can increase or shrink the window size to control the sensitivity of the measure.\n",
    "\n",
    "# 2.3   Curation vs Evolution\n",
    "\n",
    "As large corpora are published, researchers are increasingly likely to base their investigations on balanced, focused subsets that were derived from corpora produced for entirely different reasons. For instance, the Switchboard database, originally collected for speaker identification research, has since been used as the basis for published studies in speech recognition, word pronunciation, disfluency, syntax, intonation and discourse structure. The motivations for recycling linguistic corpora include the desire to save time and effort, the desire to work on material available to others for replication, and sometimes a desire to study more naturalistic forms of linguistic behavior than would be possible otherwise. The process of choosing a subset for such a study may count as a non-trivial contribution in itself.\n",
    "\n",
    "In addition to selecting an appropriate subset of a corpus, this new work could involve reformatting a text file (e.g. converting to XML), renaming files, retokenizing the text, selecting a subset of the data to enrich, and so forth. Multiple research groups might do this work independently, as illustrated in 2.2. At a later date, should someone want to combine sources of information from different versions, the task will probably be extremely onerous.\n",
    "\n",
    "![](https://www.nltk.org/images/evolution.png)\n",
    "\n",
    "The task of using derived corpora is made even more difficult by the lack of any record about how the derived version was created, and which version is the most up-to-date.\n",
    "\n",
    "An alternative to this chaotic situation is for a corpus to be centrally curated, and for committees of experts to revise and extend it at periodic intervals, considering submissions from third-parties, and publishing new releases from time to time. Print dictionaries and national corpora may be centrally curated in this way. However, for most corpora this model is simply impractical.\n",
    "\n",
    "A middle course is for the original corpus publication to have a scheme for identifying any sub-part. Each sentence, tree, or lexical entry, could have a globally unique identifier, and each token, node or field (respectively) could have a relative offset. Annotations, including segmentations, could reference the source using this identifier scheme (a method which is known as _standoff annotation_ ). This way, new annotations could be distributed independently of the source, and multiple independent annotations of the same source could be compared and updated without touching the source.\n",
    "\n",
    "If the corpus publication is provided in multiple versions, the version number or date could be part of the identification scheme. A table of correspondences between identifiers across editions of the corpus would permit any standoff annotations to be updated easily.\n",
    "\n",
    "# 3   Acquiring Data\n",
    "\n",
    "# 3.1   Obtaining Data from the Web\n",
    "The Web is a rich source of data for language analysis purposes. We have already discussed methods for accessing individual files, RSS feeds, and search engine results (see 3.1). However, in some cases we want to obtain large quantities of web text.\n",
    "\n",
    "The simplest approach is to obtain a published corpus of web text. The ACL Special Interest Group on Web as Corpus (SIGWAC) maintains a list of resources at http://www.sigwac.org.uk/. The advantage of using a well-defined web corpus is that they are documented, stable, and permit reproducible experimentation.\n",
    "\n",
    "If the desired content is localized to a particular website, there are many utilities for capturing all the accessible contents of a site, such as GNU Wget http://www.gnu.org/software/wget/. For maximal flexibility and control, a web crawler can be used, such as Heritrix http://crawler.archive.org/. Crawlers permit fine-grained control over where to look, which links to follow, and how to organize the results (Croft, Metzler, & Strohman, 2009). For example, if we want to compile a bilingual text collection having corresponding pairs of documents in each language, the crawler needs to detect the structure of the site in order to extract the correspondence between the documents, and it needs to organize the downloaded pages in such a way that the correspondence is captured. It might be tempting to write your own web-crawler, but there are dozens of pitfalls to do with detecting MIME types, converting relative to absolute URLs, avoiding getting trapped in cyclic link structures, dealing with network latencies, avoiding overloading the site or being banned from accessing the site, and so on.\n",
    "\n",
    "# 3.2   Obtaining Data from Word Processor Files\n",
    "\n",
    "Word processing software is often used in the manual preparation of texts and lexicons in projects that have limited computational infrastructure. Such projects often provide templates for data entry, though the word processing software does not ensure that the data is correctly structured. For example, each text may be required to have a title and date. Similarly, each lexical entry may have certain obligatory fields. As the data grows in size and complexity, a larger proportion of time may be spent maintaining its consistency.\n",
    "\n",
    "How can we extract the content of such files so that we can manipulate it in external programs? Moreover, how can we validate the content of these files to help authors create well-structured data, so that the quality of the data can be maximized in the context of the original authoring process?\n",
    "\n",
    "Consider a dictionary in which each entry has a part-of-speech field, drawn from a set of 20 possibilities, displayed after the pronunciation field, and rendered in 11-point bold. No conventional word processor has search or macro functions capable of verifying that all part-of-speech fields have been correctly entered and displayed. This task requires exhaustive manual checking. If the word processor permits the document to be saved in a non-proprietary format, such as text, HTML, or XML, we can sometimes write programs to do this checking automatically.\n",
    "\n",
    "Consider the following fragment of a lexical entry: \"sleep [sli:p] v.i. condition of body and mind...\". We can enter this in MSWord, then \"Save as Web Page\", then inspect the resulting HTML file:\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/2HVfNS0QNizOjziQWrFbK7CDS6b9dPGDADheCQ4C8MA.original.fullsize.png)\n",
    "\n",
    "Observe that the entry is represented as an HTML paragraph, using the <p\\> element, and that the part of speech appears inside a <span style='font-size:11.0pt'\\> element.\n",
    "    \n",
    "    The following program defines the set of legal parts-of-speech, legal_pos. Then it extracts all 11-point content from the dict.htm file and stores it in the set used_pos. Observe that the search pattern contains a parenthesized sub-expression; only the material that matches this sub-expression is returned by re.findall. Finally, the program constructs the set of illegal parts-of-speech as used_pos -\n",
    "legal_pos:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the dict.htm file \n",
    "html = \"\"\"\n",
    "<p class=MsoNormal>sleep\n",
    "  <span style='mso-spacerun:yes'> </span>\n",
    "  [<span class=SpellE>sli:p</span>]\n",
    "  <span style='mso-spacerun:yes'> </span>\n",
    "  <b><span style='font-size:11.0pt'>v.i.</span></b>\n",
    "  <span style='mso-spacerun:yes'> </span>\n",
    "  <i>a condition of body and mind ...<o:p></o:p></i>\n",
    "</p>\n",
    "\"\"\"\n",
    "f = open(\"dict.htm\", \"w\")\n",
    "f.write(html)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "legal_pos = set(['n', 'v.t.', 'v.i.', 'adj', 'det'])\n",
    "pattern = re.compile(r\"'font-size:11.0pt'>([a-z.]+)<\")\n",
    "document = open(\"dict.htm\").read()\n",
    "used_pos = set(re.findall(pattern, document))\n",
    "illegal_pos = used_pos.difference(legal_pos)\n",
    "print(list(illegal_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple program represents the tip of the iceberg. We can develop sophisticated tools to check the consistency of word processor files, and report errors so that the maintainer of the dictionary can correct the original file using the original word processor.\n",
    "\n",
    "Once we know the data is correctly formatted, we can write other programs to convert the data into a different format. The program in 3.1 strips out the HTML markup using the BeautifulSoup library, extracts the words and their pronunciations, and generates output in \"comma-separated value\" (CSV) format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def lexical_data(html_file, encoding=\"utf-8\"):\n",
    "    SEP = '_ENTRY'\n",
    "    html = open(html_file, encoding=encoding).read()\n",
    "    html = re.sub(r'<p', SEP + '<p', html)\n",
    "    text = BeautifulSoup(html, 'html.parser').get_text()\n",
    "    text = ' '.join(text.split())\n",
    "    for entry in text.split(SEP):\n",
    "        if entry.count(' ') > 2:\n",
    "            yield entry.split(' ', 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import csv\n",
    ">>> writer = csv.writer(open(\"dict1.csv\", \"w\", encoding=\"utf-8\"))\n",
    ">>> writer.writerows(lexical_data(\"dict.htm\", encoding=\"windows-1252\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3   Obtaining Data from Spreadsheets and Databases\n",
    "\n",
    "Spreadsheets are often used for acquiring wordlists or paradigms. For example, a comparative wordlist may be created using a spreadsheet, with a row for each cognate set, and a column for each language (cf. nltk.corpus.swadesh, and www.rosettaproject.org). Most spreadsheet software can export their data in CSV \"comma-separated value\" format. As we see below, it is easy for Python programs to access these using the csv module.\n",
    "\n",
    "Sometimes lexicons are stored in a full-fledged relational database. When properly normalized, these databases can ensure the validity of the data. For example, we can require that all parts-of-speech come from a specified vocabulary by declaring that the part-of-speech field is an enumerated type or a foreign key that references a separate part-of-speech table. However, the relational model requires the structure of the data (the schema) be declared in advance, and this runs counter to the dominant approach to structuring linguistic data, which is highly exploratory. Fields which were assumed to be obligatory and unique often turn out to be optional and repeatable. A relational database can accommodate this when it is fully known in advance, however if it is not, or if just about every property turns out to be optional or repeatable, the relational approach is unworkable.\n",
    "\n",
    "Nevertheless, when our goal is simply to extract the contents from a database, it is enough to dump out the tables (or SQL query results) in CSV format and load them into our program. Our program might perform a linguistically motivated query which cannot be expressed in SQL, e.g. select all words that appear in example sentences for which no dictionary entry is provided. For this task, we would need to extract enough information from a record for it to be uniquely identified, along with the headwords and example sentences. Let's suppose this information was now available in a CSV file dict.csv:\n",
    "\n",
    "    \"sleep\",\"sli:p\",\"v.i\",\"a condition of body and mind ...\"\n",
    "    \"walk\",\"wo:k\",\"v.intr\",\"progress by lifting and setting down each foot ...\"\n",
    "    \"wake\",\"weik\",\"intrans\",\"cease to sleep\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module csv:\n",
      "\n",
      "NAME\n",
      "    csv - CSV parsing and writing.\n",
      "\n",
      "DESCRIPTION\n",
      "    This module provides classes that assist in the reading and writing\n",
      "    of Comma Separated Value (CSV) files, and implements the interface\n",
      "    described by PEP 305.  Although many CSV files are simple to parse,\n",
      "    the format is not formally defined by a stable specification and\n",
      "    is subtle enough that parsing lines of a CSV file with something\n",
      "    like line.split(\",\") is bound to fail.  The module supports three\n",
      "    basic APIs: reading, writing, and registration of dialects.\n",
      "    \n",
      "    \n",
      "    DIALECT REGISTRATION:\n",
      "    \n",
      "    Readers and writers support a dialect argument, which is a convenient\n",
      "    handle on a group of settings.  When the dialect argument is a string,\n",
      "    it identifies one of the dialects previously registered with the module.\n",
      "    If it is a class or instance, the attributes of the argument are used as\n",
      "    the settings for the reader or writer:\n",
      "    \n",
      "        class excel:\n",
      "            delimiter = ','\n",
      "            quotechar = '\"'\n",
      "            escapechar = None\n",
      "            doublequote = True\n",
      "            skipinitialspace = False\n",
      "            lineterminator = '\\r\\n'\n",
      "            quoting = QUOTE_MINIMAL\n",
      "    \n",
      "    SETTINGS:\n",
      "    \n",
      "        * quotechar - specifies a one-character string to use as the \n",
      "            quoting character.  It defaults to '\"'.\n",
      "        * delimiter - specifies a one-character string to use as the \n",
      "            field separator.  It defaults to ','.\n",
      "        * skipinitialspace - specifies how to interpret whitespace which\n",
      "            immediately follows a delimiter.  It defaults to False, which\n",
      "            means that whitespace immediately following a delimiter is part\n",
      "            of the following field.\n",
      "        * lineterminator -  specifies the character sequence which should \n",
      "            terminate rows.\n",
      "        * quoting - controls when quotes should be generated by the writer.\n",
      "            It can take on any of the following module constants:\n",
      "    \n",
      "            csv.QUOTE_MINIMAL means only when required, for example, when a\n",
      "                field contains either the quotechar or the delimiter\n",
      "            csv.QUOTE_ALL means that quotes are always placed around fields.\n",
      "            csv.QUOTE_NONNUMERIC means that quotes are always placed around\n",
      "                fields which do not parse as integers or floating point\n",
      "                numbers.\n",
      "            csv.QUOTE_NONE means that quotes are never placed around fields.\n",
      "        * escapechar - specifies a one-character string used to escape \n",
      "            the delimiter when quoting is set to QUOTE_NONE.\n",
      "        * doublequote - controls the handling of quotes inside fields.  When\n",
      "            True, two consecutive quotes are interpreted as one during read,\n",
      "            and when writing, each quote character embedded in the data is\n",
      "            written as two quotes\n",
      "\n",
      "CLASSES\n",
      "    builtins.Exception(builtins.BaseException)\n",
      "        _csv.Error\n",
      "    builtins.object\n",
      "        Dialect\n",
      "            excel\n",
      "                excel_tab\n",
      "            unix_dialect\n",
      "        DictReader\n",
      "        DictWriter\n",
      "        Sniffer\n",
      "    \n",
      "    class Dialect(builtins.object)\n",
      "     |  Describe a CSV dialect.\n",
      "     |  \n",
      "     |  This must be subclassed (see csv.excel).  Valid attributes are:\n",
      "     |  delimiter, quotechar, escapechar, doublequote, skipinitialspace,\n",
      "     |  lineterminator, quoting.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  delimiter = None\n",
      "     |  \n",
      "     |  doublequote = None\n",
      "     |  \n",
      "     |  escapechar = None\n",
      "     |  \n",
      "     |  lineterminator = None\n",
      "     |  \n",
      "     |  quotechar = None\n",
      "     |  \n",
      "     |  quoting = None\n",
      "     |  \n",
      "     |  skipinitialspace = None\n",
      "    \n",
      "    class DictReader(builtins.object)\n",
      "     |  DictReader(f, fieldnames=None, restkey=None, restval=None, dialect='excel', *args, **kwds)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, f, fieldnames=None, restkey=None, restval=None, dialect='excel', *args, **kwds)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __next__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  fieldnames\n",
      "    \n",
      "    class DictWriter(builtins.object)\n",
      "     |  DictWriter(f, fieldnames, restval='', extrasaction='raise', dialect='excel', *args, **kwds)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, f, fieldnames, restval='', extrasaction='raise', dialect='excel', *args, **kwds)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  writeheader(self)\n",
      "     |  \n",
      "     |  writerow(self, rowdict)\n",
      "     |  \n",
      "     |  writerows(self, rowdicts)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Error(builtins.Exception)\n",
      "     |  Common base class for all non-exit exceptions.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Error\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class Sniffer(builtins.object)\n",
      "     |  \"Sniffs\" the format of a CSV file (i.e. delimiter, quotechar)\n",
      "     |  Returns a Dialect object.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  has_header(self, sample)\n",
      "     |  \n",
      "     |  sniff(self, sample, delimiters=None)\n",
      "     |      Returns a dialect (or None) corresponding to the sample\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class excel(Dialect)\n",
      "     |  Describe the usual properties of Excel-generated CSV files.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      excel\n",
      "     |      Dialect\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  delimiter = ','\n",
      "     |  \n",
      "     |  doublequote = True\n",
      "     |  \n",
      "     |  lineterminator = '\\r\\n'\n",
      "     |  \n",
      "     |  quotechar = '\"'\n",
      "     |  \n",
      "     |  quoting = 0\n",
      "     |  \n",
      "     |  skipinitialspace = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dialect:\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Dialect:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Dialect:\n",
      "     |  \n",
      "     |  escapechar = None\n",
      "    \n",
      "    class excel_tab(excel)\n",
      "     |  Describe the usual properties of Excel-generated TAB-delimited files.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      excel_tab\n",
      "     |      excel\n",
      "     |      Dialect\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  delimiter = '\\t'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from excel:\n",
      "     |  \n",
      "     |  doublequote = True\n",
      "     |  \n",
      "     |  lineterminator = '\\r\\n'\n",
      "     |  \n",
      "     |  quotechar = '\"'\n",
      "     |  \n",
      "     |  quoting = 0\n",
      "     |  \n",
      "     |  skipinitialspace = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dialect:\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Dialect:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Dialect:\n",
      "     |  \n",
      "     |  escapechar = None\n",
      "    \n",
      "    class unix_dialect(Dialect)\n",
      "     |  Describe the usual properties of Unix-generated CSV files.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      unix_dialect\n",
      "     |      Dialect\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  delimiter = ','\n",
      "     |  \n",
      "     |  doublequote = True\n",
      "     |  \n",
      "     |  lineterminator = '\\n'\n",
      "     |  \n",
      "     |  quotechar = '\"'\n",
      "     |  \n",
      "     |  quoting = 1\n",
      "     |  \n",
      "     |  skipinitialspace = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dialect:\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Dialect:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Dialect:\n",
      "     |  \n",
      "     |  escapechar = None\n",
      "\n",
      "FUNCTIONS\n",
      "    field_size_limit(...)\n",
      "        Sets an upper limit on parsed fields.\n",
      "            csv.field_size_limit([limit])\n",
      "        \n",
      "        Returns old limit. If limit is not given, no new limit is set and\n",
      "        the old limit is returned\n",
      "    \n",
      "    get_dialect(...)\n",
      "        Return the dialect instance associated with name.\n",
      "        dialect = csv.get_dialect(name)\n",
      "    \n",
      "    list_dialects(...)\n",
      "        Return a list of all know dialect names.\n",
      "        names = csv.list_dialects()\n",
      "    \n",
      "    reader(...)\n",
      "        csv_reader = reader(iterable [, dialect='excel']\n",
      "                                [optional keyword args])\n",
      "            for row in csv_reader:\n",
      "                process(row)\n",
      "        \n",
      "        The \"iterable\" argument can be any object that returns a line\n",
      "        of input for each iteration, such as a file object or a list.  The\n",
      "        optional \"dialect\" parameter is discussed below.  The function\n",
      "        also accepts optional keyword arguments which override settings\n",
      "        provided by the dialect.\n",
      "        \n",
      "        The returned object is an iterator.  Each iteration returns a row\n",
      "        of the CSV file (which can span multiple input lines).\n",
      "    \n",
      "    register_dialect(...)\n",
      "        Create a mapping from a string name to a dialect class.\n",
      "        dialect = csv.register_dialect(name[, dialect[, **fmtparams]])\n",
      "    \n",
      "    unregister_dialect(...)\n",
      "        Delete the name/dialect mapping associated with a string name.\n",
      "        csv.unregister_dialect(name)\n",
      "    \n",
      "    writer(...)\n",
      "        csv_writer = csv.writer(fileobj [, dialect='excel']\n",
      "                                    [optional keyword args])\n",
      "            for row in sequence:\n",
      "                csv_writer.writerow(row)\n",
      "        \n",
      "            [or]\n",
      "        \n",
      "            csv_writer = csv.writer(fileobj [, dialect='excel']\n",
      "                                    [optional keyword args])\n",
      "            csv_writer.writerows(rows)\n",
      "        \n",
      "        The \"fileobj\" argument can be any object that supports the file API.\n",
      "\n",
      "DATA\n",
      "    QUOTE_ALL = 1\n",
      "    QUOTE_MINIMAL = 0\n",
      "    QUOTE_NONE = 3\n",
      "    QUOTE_NONNUMERIC = 2\n",
      "    __all__ = ['QUOTE_MINIMAL', 'QUOTE_ALL', 'QUOTE_NONNUMERIC', 'QUOTE_NO...\n",
      "\n",
      "VERSION\n",
      "    1.0\n",
      "\n",
      "FILE\n",
      "    c:\\users\\junglebook\\anaconda3\\lib\\csv.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('dict.csv', mode='w') as f:\n",
    "    writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL, lineterminator = '\\n')\n",
    "    writer.writerow([\"sleep\",\"sli:p\",\"v.i\",\"a condition of body and mind ...\"])\n",
    "    writer.writerow([\"walk\",\"wo:k\",\"v.intr\",\"progress by lifting and setting down each foot ...\"])\n",
    "    writer.writerow([\"wake\",\"weik\",\"intrans\",\"cease to sleep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['...',\n",
       " 'a',\n",
       " 'and',\n",
       " 'body',\n",
       " 'by',\n",
       " 'cease',\n",
       " 'condition',\n",
       " 'down',\n",
       " 'each',\n",
       " 'foot',\n",
       " 'lifting',\n",
       " 'mind',\n",
       " 'of',\n",
       " 'progress',\n",
       " 'setting',\n",
       " 'to']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "lexicon = csv.reader(open('dict.csv'))\n",
    "pairs = [(lexeme, defn) for (lexeme, _, _, defn) in lexicon]\n",
    "lexemes, defns = zip(*pairs)\n",
    "defn_words = set(w for defn in defns for w in defn.split())\n",
    "sorted(defn_words.difference(lexemes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4   Converting Data Formats\n",
    "\n",
    "Annotated linguistic data rarely arrives in the most convenient format, and it is often necessary to perform various kinds of format conversion. Converting between character encodings has already been discussed (see 3.3). Here we focus on the structure of the data.\n",
    "\n",
    "In the simplest case, the input and output formats are isomorphic. For instance, we might be converting lexical data from Toolbox format to XML, and it is straightforward to transliterate the entries one at a time (4). The structure of the data is reflected in the structure of the required program: a for loop whose body takes care of a single entry.\n",
    "\n",
    "In another common case, the output is a digested form of the input, such as an inverted file index. Here it is necessary to build an index structure in memory (see 4.8), then write it to a file in the desired format. The following example constructs an index that maps the words of a dictionary definition to the corresponding lexeme [1] for each lexical entry [2], having tokenized the definition text [3], and discarded short words [4]. Once the index has been constructed we open a file and then iterate over the index entries, to write out the lines in the required format [5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> idx = nltk.Index((defn_word, lexeme) #1\n",
    "...                  for (lexeme, defn) in pairs #2\n",
    "...                  for defn_word in nltk.word_tokenize(defn) #3\n",
    "...                  if len(defn_word) > 3) #4\n",
    ">>> with open(\"dict.idx\", \"w\") as idx_file:\n",
    "...     for word in sorted(idx):\n",
    "...         idx_words = ', '.join(idx[word])\n",
    "...         idx_line = \"{}: {}\".format(word, idx_words) #5\n",
    "...         print(idx_line, file=idx_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting file dict.idx contains the following lines. (With a larger dictionary we would expect to find multiple lexemes listed for each index entry.)\n",
    "\n",
    "    body: sleep\n",
    "    cease: wake\n",
    "    condition: sleep\n",
    "    down: walk\n",
    "    each: walk\n",
    "    foot: walk\n",
    "    lifting: walk\n",
    "    mind: sleep\n",
    "    progress: walk\n",
    "    setting: walk\n",
    "    sleep: wake\n",
    "    \n",
    "In some cases, the input and output data both consist of two or more dimensions. For instance, the input might be a set of files, each containing a single column of word frequency data. The required output might be a two-dimensional table in which the original columns appear as rows. In such cases we populate an internal data structure by filling up one column at a time, then read off the data one row at a time as we write data to the output file.\n",
    "\n",
    "In the most vexing cases, the source and target formats have slightly different coverage of the domain, and information is unavoidably lost when translating between them. For example, we could combine multiple Toolbox files to create a single CSV file containing a comparative wordlist, loosing all but the \\lx field of the input files. If the CSV file was later modified, it would be a labor-intensive process to inject the changes into the original Toolbox files. A partial solution to this \"round-tripping\" problem is to associate explicit identifiers each linguistic object, and to propagate the identifiers with the objects.\n",
    "\n",
    "\n",
    "# 3.5   Deciding Which Layers of Annotation to Include\n",
    "\n",
    "Published corpora vary greatly in the richness of the information they contain. At a minimum, a corpus will typically contain at least a sequence of sound or orthographic symbols. At the other end of the spectrum, a corpus could contain a large amount of information about the syntactic structure, morphology, prosody, and semantic content of every sentence, plus annotation of discourse relations or dialogue acts. These extra layers of annotation may be just what someone needs for performing a particular data analysis task. For example, it may be much easier to find a given linguistic pattern if we can search for specific syntactic structures; and it may be easier to categorize a linguistic pattern if every word has been tagged with its sense. Here are some commonly provided annotation layers:\n",
    "\n",
    "- Word Tokenization: The orthographic form of text does not unambiguously identify its tokens. A tokenized and normalized version, in addition to the conventional orthographic version, may be a very convenient resource.\n",
    "- Sentence Segmentation: As we saw in 3, sentence segmentation can be more difficult than it seems. Some corpora therefore use explicit annotations to mark sentence segmentation.\n",
    "- Paragraph Segmentation: Paragraphs and other structural elements (headings, chapters, etc.) may be explicitly annotated.\n",
    "- Part of Speech: The syntactic category of each word in a document.\n",
    "- Syntactic Structure: A tree structure showing the constituent structure of a sentence.\n",
    "- Shallow Semantics: Named entity and coreference annotations, semantic role labels.\n",
    "- Dialogue and Discourse: dialogue act tags, rhetorical structure\n",
    "\n",
    "\n",
    "Unfortunately, there is not much consistency between existing corpora in how they represent their annotations. However, two general classes of annotation representation should be distinguished. Inline annotation modifies the original document by inserting special symbols or control sequences that carry the annotated information. For example, when part-of-speech tagging a document, the string \"fly\" might be replaced with the string \"fly/NN\", to indicate that the word fly is a noun in this context. In contrast, standoff annotation does not modify the original document, but instead creates a new file that adds annotation information using pointers that reference the original document. For example, this new document might contain the string \"<token id=8 pos='NN'/\\>\", to indicate that token 8 is a noun. (We would want to be sure that the tokenization itself was not subject to change, since it would cause such references to break silently.)\n",
    "\n",
    "# 3.6   Standards and Tools\n",
    "    \n",
    "For a corpus to be widely useful, it needs to be available in a widely supported format. However, the cutting edge of NLP research depends on new kinds of annotations, which by definition are not widely supported. In general, adequate tools for creation, publication and use of linguistic data are not widely available. Most projects must develop their own set of tools for internal use, which is no help to others who lack the necessary resources. Furthermore, we do not have adequate, generally-accepted standards for expressing the structure and content of corpora. Without such standards, general-purpose tools are impossible — though at the same time, without available tools, adequate standards are unlikely to be developed, used and accepted.\n",
    "\n",
    "One response to this situation has been to forge ahead with developing a generic format which is sufficiently expressive to capture a wide variety of annotation types (see 8 for examples). The challenge for NLP is to write programs that cope with the generality of such formats. For example, if the programming task involves tree data, and the file format permits arbitrary directed graphs, then input data must be validated to check for tree properties such as rootedness, connectedness, and acyclicity. If the input files contain other layers of annotation, the program would need to know how to ignore them when the data was loaded, but not invalidate or obliterate those layers when the tree data was saved back to the file.\n",
    "\n",
    "Another response has been to write one-off scripts to manipulate corpus formats; such scripts litter the filespaces of many NLP researchers. NLTK's corpus readers are a more systematic approach, founded on the premise that the work of parsing a corpus format should only be done once (per programming language).\n",
    "\n",
    "![](https://www.nltk.org/images/three-layer-arch.png)\n",
    "\n",
    "Instead of focussing on a common format, we believe it is more promising to develop a common interface (cf. nltk.corpus). Consider the case of treebanks, an important corpus type for work in NLP. There are many ways to store a phrase structure tree in a file. We can use nested parentheses, or nested XML elements, or a dependency notation with a (child-id, parent-id) pair on each line, or an XML version of the dependency notation, etc. However, in each case the logical structure is almost the same. It is much easier to devise a common interface that allows application programmers to write code to access tree data using methods such as children(), leaves(), depth(), and so forth. Note that this approach follows accepted practice within computer science, viz. abstract data types, object oriented design, and the three layer architecture (3.2). The last of these — from the world of relational databases — allows end-user applications to use a common model (the \"relational model\") and a common language (SQL), to abstract away from the idiosyncrasies of file storage, and allowing innovations in filesystem technologies to occur without disturbing end-user applications. In the same way, a common corpus interface insulates application programs from data formats.\n",
    "\n",
    "In this context, when creating a new corpus for dissemination, it is expedient to use an existing widely-used format wherever possible. When this is not possible, the corpus could be accompanied with software — such as an nltk.corpus module — that supports existing interface methods.\n",
    "    \n",
    "# 3.7   Special Considerations when Working with Endangered Languages\n",
    "    \n",
    "Many languages vary from town to town. Many languages have changed over time, diminished in use, etc. Linguistics are hard at work to preserve and document these langauges. NLP provides tools for collecting and curating data, with a focus on texts and lexicons. \n",
    "    \n",
    "Many languages do not have proper orthography. Because of this lexicons are created in tandem with a text collection. NLP provides software for the creation and progressive expansion/correction of lexicons. \n",
    "    \n",
    "when speakers of the language write the data, correct spelling is often overriden. Lexicons can curve this, and adding a sematic domain can help. \n",
    "    \n",
    "Lookup by pronouncation helps too! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lfnt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "mappings = [('ph', 'f'), ('ght', 't'), ('^kn', 'n'), ('qu', 'kw'),('[aeiou]+', 'a'), (r'(.)\\1', r'\\1')]\n",
    "def signature(word):\n",
    "    for patt, repl in mappings:\n",
    "        word = re.sub(patt, repl, word)\n",
    "        pieces = re.findall('[^aeiou]+', word)\n",
    "    return ''.join(char for piece in pieces for char in sorted(piece))[:8]\n",
    "signature('illefent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anicular',\n",
       " 'inocular',\n",
       " 'nucellar',\n",
       " 'nuclear',\n",
       " 'unicolor',\n",
       " 'uniocular',\n",
       " 'unocular']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, we create a mapping from signatures to words, for all the words in our lexicon. \n",
    "# We can use this to get candidate corrections for a given input word \n",
    "signatures = nltk.Index((signature(w), w) for w in nltk.corpus.words.words())\n",
    "signatures[signature('nuculerr')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['olefiant', 'elephant', 'oliphant', 'elephanta']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we should rank the results in terms of similarity with the original word.\n",
    "#This is done by the function rank(). \n",
    "# The only remaining function provides a simple interface to the user:\n",
    "def rank(word, wordlist):\n",
    "    ranked = sorted((nltk.edit_distance(word, w), w) for w in wordlist)\n",
    "    return [word for (_, word) in ranked]\n",
    "def fuzzy_spell(word):\n",
    "    sig = signature(word)\n",
    "    if sig in signatures:\n",
    "        return rank(word, signatures[sig])\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "fuzzy_spell('illefent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just one illustration where a simple program can facilitate access to lexical data in a context where the writing system of a language may not be standardized, or where users of the language may not have a good command of spellings. \n",
    "\n",
    "Other simple applications of NLP in this area include: building indexes to facilitate access to data, gleaning wordlists from texts, locating examples of word usage in constructing a lexicon, detecting prevalent or exceptional patterns in poorly understood data, and performing specialized validation on data created using various linguistic software tools. We will return to the last of these in 5.\n",
    "\n",
    "# 4   Working with XML\n",
    "\n",
    "The **Extensible Markup Language** (XML) provides a framework for designing domain-specific markup languages. It is sometimes used for representing annotated text and for lexical resources. \n",
    "\n",
    "Unlike HTML with its predefined tags, XML permits us to make up our own tags. Unlike a database, XML permits us to create data without first specifying its structure, and it permits us to have optional and repeatable elements. \n",
    "\n",
    "# 4.1   Using XML for Linguistic Structures\n",
    "\n",
    "Thanks to its flexibility and extensibility, XML is a natural choice for representing linguistic structures. Here's an example of a simple lexical entry.\n",
    "\n",
    "    <entry>\n",
    "      <headword>whale</headword>\n",
    "      <pos>noun</pos>\n",
    "      <gloss>any of the larger cetacean mammals having a streamlined\n",
    "        body and breathing through a blowhole on the head</gloss>\n",
    "    </entry>\n",
    "    \n",
    "It consists of a series of XML tags enclosed in angle brackets. Each opening tag, like <gloss\\> is matched with a closing tag, like </gloss\\>; together they constitute an XML element. The above example has been laid out nicely using whitespace, but it could equally have been put on a single, long line. Our approach to processing XML will usually not be sensitive to whitespace. In order for XML to be well formed, all opening tags must have corresponding closing tags, at the same level of nesting (i.e. the XML document must be a well-formed tree).\n",
    "\n",
    "XML permits us to repeat elements, e.g. to add another gloss field as we see below. We will use different whitespace to underscore the point that layout does not matter.\n",
    "\n",
    "    <entry><headword>whale</headword><pos>noun</pos><gloss>any of the\n",
    "    larger cetacean mammals having a streamlined body and breathing\n",
    "    through a blowhole on the head</gloss><gloss>a very large person;\n",
    "    impressive in size or qualities</gloss></entry>\n",
    "    \n",
    "A further step might be to link our lexicon to some external resource, such as WordNet, using external identifiers. In (4) we group the gloss and a synset identifier inside a new element which we have called \"sense\"\n",
    "\n",
    "    <entry>\n",
    "      <headword>whale</headword>\n",
    "      <pos>noun</pos>\n",
    "      <sense>\n",
    "        <gloss>any of the larger cetacean mammals having a streamlined\n",
    "          body and breathing through a blowhole on the head</gloss>\n",
    "        <synset>whale.n.02</synset>\n",
    "      </sense>\n",
    "      <sense>\n",
    "        <gloss>a very large person; impressive in size or qualities</gloss>\n",
    "        <synset>giant.n.04</synset>\n",
    "      </sense>\n",
    "    </entry>\n",
    "\n",
    "Alternatively, we could have represented the synset identifier using an XML attribute, without the need for any nested structure, as below\n",
    "\n",
    "   \n",
    "   \n",
    "    <entry>\n",
    "      <headword>whale</headword>\n",
    "      <pos>noun</pos>\n",
    "      <gloss synset=\"whale.n.02\">any of the larger cetacean mammals having\n",
    "          a streamlined body and breathing through a blowhole on the head</gloss>\n",
    "      <gloss synset=\"giant.n.04\">a very large person; impressive in size or\n",
    "          qualities</gloss>\n",
    "    </entry>\n",
    "    \n",
    "This illustrates some of the flexibility of XML. If it seems somewhat arbitrary that's because it is! Following the rules of XML we can invent new attribute names, and nest them as deeply as we like. We can repeat elements, leave them out, and put them in a different order each time. \n",
    "\n",
    "We can have fields whose presence depends on the value of some other field, e.g. if the part of speech is \"verb\", then the entry can have a past_tense element to hold the past tense of the verb, but if the part of speech is \"noun\" no past_tense element is permitted. To impose some order over all this freedom, we can constrain the structure of an XML file using a \"schema,\" which is a declaration akin to a context free grammar. Tools exist for testing the validity of an XML file with respect to a schema.\n",
    "\n",
    "# 4.2   The Role of XML\n",
    "\n",
    "We can use XML to represent many kinds of linguistic information. However, the flexibility comes at a price. Each time we introduce a complication, such as by permitting an element to be optional or repeated, we make more work for any program that accesses the data. We also make it more difficult to check the validity of the data, or to interrogate the data using one of the XML query languages.\n",
    "\n",
    "Thus, using XML to represent linguistic structures does not magically solve the data modeling problem. We still have to work out how to structure the data, then define that structure with a schema, and then write programs to read and write the format and convert it to other formats. \n",
    "\n",
    "Similarly, we still need to follow some standard principles concerning data normalization. It is wise to avoid making duplicate copies of the same information, so that we don't end up with inconsistent data when only one copy is changed. For example, a cross-reference that was represented as <xref>headword</xref> would duplicate the storage of the headword of some other lexical entry, and the link would break if the copy of the string at the other location was modified. Existential dependencies between information types need to be modeled, so that we can't create elements without a home. For example, if sense definitions cannot exist independently of a lexical entry, the sense element can be nested inside the entry element. Many-to-many relations need to be abstracted out of hierarchical structures. For example, if a word can have many corresponding senses, and a sense can have several corresponding words, then both words and senses must be enumerated separately, as must the list of (word, sense) pairings. This complex structure might even be split across three separate XML files.\n",
    "\n",
    "As we can see, although XML provides us with a convenient format accompanied by an extensive collection of tools, it offers no panacea (a solution or remedy for all difficulties or diseases).\n",
    "\n",
    "# 4.3   The ElementTree Interface\n",
    "\n",
    "Python's ElementTree module provides a convenient way to access data stored in XML files. ElementTree is part of Python's standard library.\n",
    "\n",
    "We will illustrate the use of ElementTree using a collection of Shakespeare plays that have been formatted using XML. Let's load the XML file and inspect the raw data, first at the top of the file [1], where we see some XML headers and the name of a schema called play.dtd, followed by the root element PLAY. We pick it up again at the start of Act 1 [2]. .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\"?>\n",
      "<?xml-stylesheet type=\"text/css\" href=\"shakes.css\"?>\n",
      "<!-- <!DOCTYPE PLAY SYSTEM \"play.dtd\"> -->\n",
      "\n",
      "<PLAY>\n",
      "<TITLE>The Merchant of Venice</TITLE>\n"
     ]
    }
   ],
   "source": [
    ">>> merchant_file = nltk.data.find('corpora/shakespeare/merchant.xml')\n",
    ">>> raw = open(merchant_file).read()\n",
    ">>> print(raw[:163]) #[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TITLE>ACT I</TITLE>\n",
      "\n",
      "<SCENE><TITLE>SCENE I.  Venice. A street.</TITLE>\n",
      "<STAGEDIR>Enter ANTONIO, SALARINO, and SALANIO</STAGEDIR>\n",
      "\n",
      "<SPEECH>\n",
      "<SPEAKER>ANTONIO</SPEAKER>\n",
      "<LINE>In sooth, I know not why I am so sad:</LINE>\n"
     ]
    }
   ],
   "source": [
    " print(raw[1789:2006]) #[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just accessed the XML data as a string. As we can see, the string at the start of Act 1 contains XML tags for title, scene, stage directions, and so forth.\n",
    "\n",
    "The next step is to process the file contents as structured XML data, using ElementTree. We are processing a file (a multi-line string) and building a tree, so its not surprising that the method name is parse [1]. The variable merchant contains an XML element PLAY [2]. This element has internal structure; we can use an index to get its first child, a TITLE element [3]. We can also see the text content of this element, the title of the play [4]. To get a list of all the child elements, we use the getchildren() method [5].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'PLAY' at 0x000001D90C612EA8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from xml.etree.ElementTree import ElementTree\n",
    ">>> merchant = ElementTree().parse(merchant_file) #[1]\n",
    ">>> merchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'TITLE' at 0x000001D90C612228>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " merchant[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dramatis Personae'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JungleBook\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Element 'TITLE' at 0x000001D90C612228>,\n",
       " <Element 'PERSONA' at 0x000001D90C612278>,\n",
       " <Element 'PGROUP' at 0x000001D90C6122C8>,\n",
       " <Element 'PERSONA' at 0x000001D90C612408>,\n",
       " <Element 'PERSONA' at 0x000001D90C6124A8>,\n",
       " <Element 'PGROUP' at 0x000001D90C6124F8>,\n",
       " <Element 'PERSONA' at 0x000001D90C6126D8>,\n",
       " <Element 'PERSONA' at 0x000001D90C612778>,\n",
       " <Element 'PERSONA' at 0x000001D90C6127C8>,\n",
       " <Element 'PERSONA' at 0x000001D90C612868>,\n",
       " <Element 'PERSONA' at 0x000001D90C6128B8>,\n",
       " <Element 'PERSONA' at 0x000001D90C612908>,\n",
       " <Element 'PGROUP' at 0x000001D90C6129A8>,\n",
       " <Element 'PERSONA' at 0x000001D90C612AE8>,\n",
       " <Element 'PERSONA' at 0x000001D90C612B38>,\n",
       " <Element 'PERSONA' at 0x000001D90C612BD8>,\n",
       " <Element 'PERSONA' at 0x000001D90C612C78>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant.getchildren() # 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACT IV'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The play consists of a title, the personae, a scene description, a subtitle, and five acts. \n",
    "# Each act has a title and some scenes, and each scene consists of speeches which are made up of lines, \n",
    "# a structure with four levels of nesting. Let's dig down into Act IV:\n",
    "merchant[-2][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'SCENE' at 0x000001D90C678E08>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant[-2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SCENE I.  Venice. A court of justice.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant[-2][1][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act 3 Scene 2 Speech 9: Let music sound while he doth make his choice;\n",
      "Act 3 Scene 2 Speech 9: Fading in music: that the comparison\n",
      "Act 3 Scene 2 Speech 9: And what is music then? Then music is\n",
      "Act 5 Scene 1 Speech 23: And bring your music forth into the air.\n",
      "Act 5 Scene 1 Speech 23: Here will we sit and let the sounds of music\n",
      "Act 5 Scene 1 Speech 23: And draw her home with music.\n",
      "Act 5 Scene 1 Speech 24: I am never merry when I hear sweet music.\n",
      "Act 5 Scene 1 Speech 25: Or any air of music touch their ears,\n",
      "Act 5 Scene 1 Speech 25: By the sweet power of music: therefore the poet\n",
      "Act 5 Scene 1 Speech 25: But music for the time doth change his nature.\n",
      "Act 5 Scene 1 Speech 25: The man that hath no music in himself,\n",
      "Act 5 Scene 1 Speech 25: Let no such man be trusted. Mark the music.\n",
      "Act 5 Scene 1 Speech 29: It is your music, madam, of the house.\n",
      "Act 5 Scene 1 Speech 32: No better a musician than the wren.\n"
     ]
    }
   ],
   "source": [
    "# Although we can access the entire tree this way, it is more convenient to search for sub-elements\n",
    "# with particular names. Recall that the elements at the top level have several types. \n",
    "# We can iterate over just the types we are interested in (such as the acts), \n",
    "# using merchant.findall('ACT'). Here's an example of doing such tag-specific searches at every level of nesting:\n",
    "for i, act in enumerate(merchant.findall('ACT')):\n",
    "    for j, scene in enumerate(act.findall('SCENE')):\n",
    "        for k, speech in enumerate(scene.findall('SPEECH')):\n",
    "            for line in speech.findall('LINE'):\n",
    "                if 'music' in str(line.text):\n",
    "                    print(\"Act %d Scene %d Speech %d: %s\" % (i+1, j+1, k+1, line.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PORTIA', 117),\n",
       " ('SHYLOCK', 79),\n",
       " ('BASSANIO', 73),\n",
       " ('GRATIANO', 48),\n",
       " ('ANTONIO', 47)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead of navigating each step of the way down the hierarchy, we can search for particular embedded elements. \n",
    "# For example, let's examine the sequence of speakers. \n",
    "#We can use a frequency distribution to see who has the most to say:\n",
    "from collections import Counter\n",
    "speaker_seq = [s.text for s in merchant.findall('ACT/SCENE/SPEECH/SPEAKER')]\n",
    "speaker_freq = Counter(speaker_seq)\n",
    "top5 = speaker_freq.most_common(5)\n",
    "top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ANTO BASS GRAT  OTH PORT SHYL \n",
      "ANTO    0   11    4   11    9   12 \n",
      "BASS   10    0   11   10   26   16 \n",
      "GRAT    6    8    0   19    9    5 \n",
      " OTH    8   16   18  153   52   25 \n",
      "PORT    7   23   13   53    0   21 \n",
      "SHYL   15   15    2   26   21    0 \n"
     ]
    }
   ],
   "source": [
    "# We can also look for patterns in who follows who in the dialogues. \n",
    "# Since there's 23 speakers, we need to reduce the \"vocabulary\" to a manageable size first\n",
    "from collections import defaultdict\n",
    "abbreviate = defaultdict(lambda: 'OTH')\n",
    "for speaker, _ in top5:\n",
    "    abbreviate[speaker] = speaker[:4]\n",
    "speaker_seq2 = [abbreviate[speaker] for speaker in speaker_seq]\n",
    "cfd = nltk.ConditionalFreqDist(nltk.bigrams(speaker_seq2))\n",
    "cfd.tabulate()\n",
    "# largest value suggests that Portia and Bassanio have the most frequent interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4   Using ElementTree for Accessing Toolbox Data\n",
    "\n",
    "In chpater 4 we saw a simple interface for accessing Toolbox data, a popular and well-established format used by linguists for managing data. In this section we discuss a variety of techniques for manipulating Toolbox data in ways that are not supported by the Toolbox software. The methods we discuss could be applied to other record-structured data, regardless of the actual file format.\n",
    "\n",
    "We can use the toolbox.xml() method to access a Toolbox file and load it into an elementtree object. This file contains a lexicon for the Rotokas language of Papua New Guinea.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from nltk.corpus import toolbox\n",
    ">>> lexicon = toolbox.xml('rotokas.dic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to access the contents of the lexicon object, by indexes and by paths. Indexes use the familiar syntax, thus lexicon[3] returns entry number 3 (which is actually the fourth entry counting from zero); lexicon[3][0] returns its first field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'lx' at 0x000001D90C6A4598>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lx'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon[3][0].tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kaa'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon[3][0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way to access the contents of the lexicon object uses paths. The lexicon is a series of record objects, each containing a series of field objects, such as lx and ps. We can conveniently address all of the lexemes using the path record/lx. Here we use the findall() function to search for any matches to the path record/lx, and we access the text content of the element, normalizing it to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kaa',\n",
       " 'kaa',\n",
       " 'kaa',\n",
       " 'kaakaaro',\n",
       " 'kaakaaviko',\n",
       " 'kaakaavo',\n",
       " 'kaakaoko',\n",
       " 'kaakasi',\n",
       " 'kaakau',\n",
       " 'kaakauko',\n",
       " 'kaakito',\n",
       " 'kaakuupato',\n",
       " 'kaaova',\n",
       " 'kaapa',\n",
       " 'kaapea',\n",
       " 'kaapie',\n",
       " 'kaapie',\n",
       " 'kaapiepato',\n",
       " 'kaapisi',\n",
       " 'kaapisivira',\n",
       " 'kaapo',\n",
       " 'kaapopato',\n",
       " 'kaara',\n",
       " 'kaare',\n",
       " 'kaareko',\n",
       " 'kaarekopie',\n",
       " 'kaareto',\n",
       " 'kaareva',\n",
       " 'kaava',\n",
       " 'kaavaaua',\n",
       " 'kaaveaka',\n",
       " 'kaaveakapie',\n",
       " 'kaaveakapievira',\n",
       " 'kaaveakavira',\n",
       " 'kae',\n",
       " 'kae',\n",
       " 'kaekae',\n",
       " 'kaekae',\n",
       " 'kaekaearo',\n",
       " 'kaekaeo',\n",
       " 'kaekaesoto',\n",
       " 'kaekaevira',\n",
       " 'kaekeru',\n",
       " 'kaepaa',\n",
       " 'kaepie',\n",
       " 'kaepie',\n",
       " 'kaepievira',\n",
       " 'kaereasi',\n",
       " 'kaereasivira',\n",
       " 'kaetu',\n",
       " 'kaetupie',\n",
       " 'kaetuvira',\n",
       " 'kaeviro',\n",
       " 'kagave',\n",
       " 'kaie',\n",
       " 'kaiea',\n",
       " 'kaikaio',\n",
       " 'kaio',\n",
       " 'kaipori',\n",
       " 'kaiporipie',\n",
       " 'kaiporivira',\n",
       " 'kairi',\n",
       " 'kairiro',\n",
       " 'kairo',\n",
       " 'kaita',\n",
       " 'kaitutu',\n",
       " 'kaitutupie',\n",
       " 'kaitutuvira',\n",
       " 'kakae',\n",
       " 'kakae',\n",
       " 'kakae',\n",
       " 'kakaevira',\n",
       " 'kakapikoa',\n",
       " 'kakapikoto',\n",
       " 'kakapu',\n",
       " 'kakapua',\n",
       " 'kakara',\n",
       " 'kakarapaia',\n",
       " 'kakarau',\n",
       " 'kakarera',\n",
       " 'kakata',\n",
       " 'kakate',\n",
       " 'kakatuara',\n",
       " 'kakau',\n",
       " 'kakauoa',\n",
       " 'kakavea',\n",
       " 'kakavoro',\n",
       " 'kakavu',\n",
       " 'kakeoto',\n",
       " 'kaki',\n",
       " 'kaki',\n",
       " 'kakiaki',\n",
       " 'kakiri',\n",
       " 'kakiua',\n",
       " 'kaku',\n",
       " 'kakua',\n",
       " 'kakuaku',\n",
       " 'kakupaa',\n",
       " 'kakuparei',\n",
       " 'kakupato',\n",
       " 'kakupie',\n",
       " 'kakupute',\n",
       " 'kakutauo',\n",
       " 'kakuto',\n",
       " 'kakutuiato',\n",
       " 'kakuva',\n",
       " 'kakuvira',\n",
       " 'kameoro',\n",
       " 'kandora',\n",
       " 'kaokao',\n",
       " 'kaokaoara',\n",
       " 'kaokaoto',\n",
       " 'kapa',\n",
       " 'kapa',\n",
       " 'kapaava',\n",
       " 'kapai',\n",
       " 'kapara',\n",
       " 'kaparu',\n",
       " 'kaparuvira',\n",
       " 'kapatau',\n",
       " 'kapatoro',\n",
       " 'kapatoroto',\n",
       " 'kape',\n",
       " 'kapeaa',\n",
       " 'kapeaa',\n",
       " 'kapeaavira',\n",
       " 'kapekape',\n",
       " 'kapekapevira',\n",
       " 'kapere',\n",
       " 'kaperepie',\n",
       " 'kapi',\n",
       " 'kapiaa',\n",
       " 'kapiaavira',\n",
       " 'kapikapi',\n",
       " 'kapiro',\n",
       " 'kapiroa',\n",
       " 'kapiroko',\n",
       " 'kapisi',\n",
       " 'kapisito',\n",
       " 'kapiu',\n",
       " 'kapiua',\n",
       " 'kapo',\n",
       " 'kapoa',\n",
       " 'kapokao',\n",
       " 'kapokapo',\n",
       " 'kapokapoa',\n",
       " 'kapokapora',\n",
       " 'kapokaporo',\n",
       " 'kapokari',\n",
       " 'kapokarito',\n",
       " 'kapokoa',\n",
       " 'kapoo',\n",
       " 'kapooto',\n",
       " 'kapoovira',\n",
       " 'kapopaa',\n",
       " 'kaporo',\n",
       " 'kaporo',\n",
       " 'kaporopa',\n",
       " 'kaporoto',\n",
       " 'kapoto',\n",
       " 'kapu',\n",
       " 'kapua',\n",
       " 'kapua',\n",
       " 'kapuapato',\n",
       " 'kapuapie',\n",
       " 'kapuasisi',\n",
       " 'kapupie',\n",
       " 'kapupiea',\n",
       " 'kapupiepaa',\n",
       " 'kapuu',\n",
       " 'kapuupie',\n",
       " 'kapuupiepa',\n",
       " 'kara',\n",
       " 'kara',\n",
       " 'karaava',\n",
       " 'karakarao',\n",
       " 'karakaraoa',\n",
       " 'karakaraoto',\n",
       " 'karakaraovira',\n",
       " 'karakaroto',\n",
       " 'karakuku',\n",
       " 'karaova',\n",
       " 'karapi',\n",
       " 'karapivira',\n",
       " 'karara',\n",
       " 'karata',\n",
       " 'karato',\n",
       " 'karavau',\n",
       " 'karavisi',\n",
       " 'karavisito',\n",
       " 'karavuru',\n",
       " 'kare',\n",
       " 'kare',\n",
       " 'karekare',\n",
       " 'karekare',\n",
       " 'karekarererava',\n",
       " 'karekareto',\n",
       " 'kareke',\n",
       " 'karekepie',\n",
       " 'karekova',\n",
       " 'kareo',\n",
       " 'kareovira',\n",
       " 'karepie',\n",
       " 'karepieto',\n",
       " 'karepirie',\n",
       " 'kari',\n",
       " 'karia',\n",
       " 'kariava',\n",
       " 'karikari',\n",
       " 'karirapa',\n",
       " 'karisito',\n",
       " 'karivai',\n",
       " 'karivaito',\n",
       " 'karivara',\n",
       " 'karo',\n",
       " 'karokaropo',\n",
       " 'karopato',\n",
       " 'karopo',\n",
       " 'karot',\n",
       " 'karoto',\n",
       " 'karova',\n",
       " 'karu',\n",
       " 'karuka',\n",
       " 'karukaru',\n",
       " 'karukava',\n",
       " 'karuru',\n",
       " 'karutu',\n",
       " 'karuvira',\n",
       " 'kasi',\n",
       " 'kasi',\n",
       " 'kasi',\n",
       " 'kasiarao',\n",
       " 'kasiava',\n",
       " 'kasikasi',\n",
       " 'kasikasiua',\n",
       " 'kasipie',\n",
       " 'kasipu',\n",
       " 'kasipupie',\n",
       " 'kasipuvira',\n",
       " 'kasirao',\n",
       " 'kasiraopie',\n",
       " 'kasiraovira',\n",
       " 'kasiu',\n",
       " 'kasiura',\n",
       " 'kasivari',\n",
       " 'kasuari',\n",
       " 'kata',\n",
       " 'katai',\n",
       " 'kataitoarei',\n",
       " 'katakatai',\n",
       " 'katakataivira',\n",
       " 'kataraua',\n",
       " 'katarauto',\n",
       " 'katavira',\n",
       " 'katokato',\n",
       " 'katokatoto',\n",
       " 'katokatovira',\n",
       " 'katokoi',\n",
       " 'katopato',\n",
       " 'katoto',\n",
       " 'katuara',\n",
       " 'katuarato',\n",
       " 'katukatu',\n",
       " 'katuta',\n",
       " 'kau',\n",
       " 'kaukau',\n",
       " 'kaukaupie',\n",
       " 'kaukauvira',\n",
       " 'kaukovo',\n",
       " 'kauo',\n",
       " 'kauokauo',\n",
       " 'kaureo',\n",
       " 'kaureoto',\n",
       " 'kausiopa',\n",
       " 'kausiovira',\n",
       " 'kava',\n",
       " 'kavakavau',\n",
       " 'kavatao',\n",
       " 'kavatara',\n",
       " 'kavau',\n",
       " 'kavau',\n",
       " 'kavau asiava',\n",
       " 'kave',\n",
       " 'kavee',\n",
       " 'kaveepaa',\n",
       " 'kaverui',\n",
       " 'kaveruko',\n",
       " 'kavesi',\n",
       " 'kavikavi',\n",
       " 'kavikaviru',\n",
       " 'kavikaviru',\n",
       " 'kaviko',\n",
       " 'kavikoa',\n",
       " 'kaviru',\n",
       " 'kaviru',\n",
       " 'kaviruto',\n",
       " 'kaviruvira',\n",
       " 'kavo',\n",
       " 'kavokavo',\n",
       " 'kavokavoa',\n",
       " 'kavokavoto',\n",
       " 'kavora',\n",
       " 'kavorato',\n",
       " 'kavori',\n",
       " 'kavori',\n",
       " 'kavorou',\n",
       " 'kavovoa',\n",
       " 'kavovovira',\n",
       " 'kavu',\n",
       " 'kavu',\n",
       " 'kavuava',\n",
       " 'kavupie',\n",
       " 'kavura',\n",
       " 'kavurao',\n",
       " 'kavusi',\n",
       " 'kavuvo',\n",
       " 'kea',\n",
       " 'keakea',\n",
       " 'keakeato',\n",
       " 'keari',\n",
       " 'kearito',\n",
       " 'keavira',\n",
       " 'kee',\n",
       " 'keekee',\n",
       " 'keekeepa',\n",
       " 'keekeeri',\n",
       " 'keekeerito',\n",
       " 'keera',\n",
       " 'keeriva',\n",
       " 'keesi',\n",
       " 'keetaa',\n",
       " 'keevuru',\n",
       " 'keevuruvira',\n",
       " 'kegi',\n",
       " 'kei',\n",
       " 'keke',\n",
       " 'keke',\n",
       " 'kekepie',\n",
       " 'kekeputu',\n",
       " 'kekeputuvira',\n",
       " 'kekeraokovira',\n",
       " 'kekesopa',\n",
       " 'kekevoto',\n",
       " 'kekevotovira',\n",
       " 'kekira',\n",
       " 'keo',\n",
       " 'keoka',\n",
       " 'keopa',\n",
       " 'keovira',\n",
       " 'kepa',\n",
       " 'kepa toupato',\n",
       " 'kepetai',\n",
       " 'kepi',\n",
       " 'kepia',\n",
       " 'kepikepi',\n",
       " 'kepiko',\n",
       " 'kepiriko',\n",
       " 'kepiro',\n",
       " 'kepisi',\n",
       " 'kepisiva',\n",
       " 'kepita',\n",
       " 'kepitai',\n",
       " 'kepito',\n",
       " 'kepo',\n",
       " 'kepoi',\n",
       " 'keposi',\n",
       " 'kepoto',\n",
       " 'kera',\n",
       " 'kerakera',\n",
       " 'kerari',\n",
       " 'keraria',\n",
       " 'kerau',\n",
       " 'kerauto',\n",
       " 'keravisi',\n",
       " 'keravisia',\n",
       " 'keravo',\n",
       " 'kereaka',\n",
       " 'kerekoi',\n",
       " 'kerere',\n",
       " 'kerereua',\n",
       " 'kerete',\n",
       " 'kerete',\n",
       " 'kerevaru',\n",
       " 'keri',\n",
       " 'keriaka',\n",
       " 'kerikerisi',\n",
       " 'kerio',\n",
       " 'kerioua',\n",
       " 'keripaara',\n",
       " 'keripato',\n",
       " 'kerisi',\n",
       " 'kerisito',\n",
       " 'kerisivira',\n",
       " 'keritara',\n",
       " 'keriva',\n",
       " 'keroroi',\n",
       " 'kerosiri',\n",
       " 'keru',\n",
       " 'keru',\n",
       " 'kerui',\n",
       " 'keruiato',\n",
       " 'keruito',\n",
       " 'kerupiua',\n",
       " 'keruria',\n",
       " 'keruriato',\n",
       " 'keruriavira',\n",
       " 'kesi',\n",
       " 'kesie',\n",
       " 'kesievira',\n",
       " 'kesikea vaaguru',\n",
       " 'kesio',\n",
       " 'kesioto',\n",
       " 'kesivira',\n",
       " 'keta',\n",
       " 'ketaka',\n",
       " 'ketakaa',\n",
       " 'ketato',\n",
       " 'ketoo',\n",
       " 'ketoo',\n",
       " 'ketoopie',\n",
       " 'ketoopieara',\n",
       " 'ketoroa',\n",
       " 'ketu',\n",
       " 'kevaita',\n",
       " 'kevaita',\n",
       " 'kevaitato',\n",
       " 'kevira',\n",
       " 'kevisi',\n",
       " 'kevoisi',\n",
       " 'kevoisivira',\n",
       " 'kie',\n",
       " 'kii',\n",
       " 'kiikariko',\n",
       " 'kiipie',\n",
       " 'kiire',\n",
       " 'kiire',\n",
       " 'kiiru',\n",
       " 'kiiuto',\n",
       " 'kiki',\n",
       " 'kiki',\n",
       " 'kikipi',\n",
       " 'kikipisi',\n",
       " 'kikira',\n",
       " 'kikiraeko',\n",
       " 'kikisi',\n",
       " 'kikisikae',\n",
       " 'kikisiova',\n",
       " 'kikitausi',\n",
       " 'kikoo',\n",
       " 'kilia',\n",
       " 'kio',\n",
       " 'kipa',\n",
       " 'kipapie',\n",
       " 'kipe',\n",
       " 'kipekipe',\n",
       " 'kipekipea',\n",
       " 'kipeto',\n",
       " 'kipu',\n",
       " 'kipukipu',\n",
       " 'kipupaa',\n",
       " 'kipupato',\n",
       " 'kipuvira',\n",
       " 'kira',\n",
       " 'kirava',\n",
       " 'kire',\n",
       " 'kiri',\n",
       " 'kirikaokao',\n",
       " 'kirioto',\n",
       " 'kiro',\n",
       " 'kirokiro',\n",
       " 'kirokiro',\n",
       " 'kirokiropato',\n",
       " 'kiroko',\n",
       " 'kiru',\n",
       " 'kirukiru',\n",
       " 'kirukirua',\n",
       " 'kirupato',\n",
       " 'kitoiva',\n",
       " 'kitu',\n",
       " 'kitukitu',\n",
       " 'kiu',\n",
       " 'kiupie',\n",
       " 'kiuto',\n",
       " 'kiuve',\n",
       " 'kiuvu',\n",
       " 'koa',\n",
       " 'koai',\n",
       " 'koakoa',\n",
       " 'koakoa',\n",
       " 'koara',\n",
       " 'koarao',\n",
       " 'koaraua',\n",
       " 'koarava',\n",
       " 'koasio',\n",
       " 'koata',\n",
       " 'koatapie',\n",
       " 'koauve',\n",
       " 'koavaato',\n",
       " 'koe',\n",
       " 'koea',\n",
       " 'koekoe',\n",
       " 'koekoeto',\n",
       " 'koepato',\n",
       " 'koeta',\n",
       " 'koetaova',\n",
       " 'koetapie',\n",
       " 'koetava',\n",
       " 'koetavira',\n",
       " 'koeto',\n",
       " 'kogo',\n",
       " 'kogova',\n",
       " 'koi',\n",
       " 'koie',\n",
       " 'koie',\n",
       " 'koike',\n",
       " 'koiketo',\n",
       " 'koikevira',\n",
       " 'koikoi',\n",
       " 'koikoipato',\n",
       " 'koikoipie',\n",
       " 'koikoito',\n",
       " 'koisi',\n",
       " 'koisiva',\n",
       " 'koivira',\n",
       " 'koka',\n",
       " 'kokai',\n",
       " 'kokara',\n",
       " 'kokaraa',\n",
       " 'kokarapato',\n",
       " 'koke',\n",
       " 'kokee',\n",
       " 'kokepato',\n",
       " 'kokepato',\n",
       " 'kokepie',\n",
       " 'kokerao',\n",
       " 'kokeriva',\n",
       " 'kokeu',\n",
       " 'kokeva',\n",
       " 'koki',\n",
       " 'kokio',\n",
       " 'kokipaia',\n",
       " 'kokito',\n",
       " 'kokivira',\n",
       " 'koko',\n",
       " 'koko',\n",
       " 'kokoi',\n",
       " 'kokoisi',\n",
       " 'kokokoru',\n",
       " 'kokoo',\n",
       " 'kokooko',\n",
       " 'kokookoa',\n",
       " 'kokoote',\n",
       " 'kokootu',\n",
       " 'kokopa',\n",
       " 'kokopakou',\n",
       " 'kokopeko',\n",
       " 'kokopekovira',\n",
       " 'kokopeoto',\n",
       " 'kokopuoto',\n",
       " 'kokopuovira',\n",
       " 'kokopuru',\n",
       " 'kokopuvira',\n",
       " 'kokora',\n",
       " 'kokorai',\n",
       " 'kokorato',\n",
       " 'kokori',\n",
       " 'kokorivira',\n",
       " 'kokoro',\n",
       " 'kokoroki',\n",
       " 'kokoroku',\n",
       " 'kokorokupie',\n",
       " 'kokoropato',\n",
       " 'kokorosi',\n",
       " 'kokorovira',\n",
       " 'kokoru',\n",
       " 'kokoruu',\n",
       " 'kokoruu',\n",
       " 'kokosi',\n",
       " 'kokosi',\n",
       " 'kokosiria',\n",
       " 'kokosito',\n",
       " 'kokosiva',\n",
       " 'kokotagoe',\n",
       " 'kokote',\n",
       " 'kokoto',\n",
       " 'kokotu',\n",
       " 'kokotua',\n",
       " 'kokotuo',\n",
       " 'kokovae',\n",
       " 'kokovaeva',\n",
       " 'kokovara',\n",
       " 'kokovara',\n",
       " 'kokovaravira',\n",
       " 'kokovu',\n",
       " 'kokovua',\n",
       " 'kokovua',\n",
       " 'kokovua',\n",
       " 'kokovupaparie',\n",
       " 'kokovurito',\n",
       " 'koku',\n",
       " 'kokuoku',\n",
       " 'kokureko',\n",
       " 'kokuuto',\n",
       " 'kooe',\n",
       " 'kookaa',\n",
       " 'kookai',\n",
       " 'kookoo',\n",
       " 'kookooia',\n",
       " 'kookoopeko',\n",
       " 'kookoopi',\n",
       " 'kooku',\n",
       " 'kookuto',\n",
       " 'kookuvira',\n",
       " 'koopi',\n",
       " 'koopipi',\n",
       " 'koora',\n",
       " 'kooroo',\n",
       " 'koorooto',\n",
       " 'koorooto',\n",
       " 'kooroovira',\n",
       " 'kooru',\n",
       " 'koota',\n",
       " 'kootopa',\n",
       " 'kootutu',\n",
       " 'koou',\n",
       " 'kooupato',\n",
       " 'koova',\n",
       " 'koova',\n",
       " 'koovoto',\n",
       " 'koovotova',\n",
       " 'kopa',\n",
       " 'kopakai',\n",
       " 'kopakava',\n",
       " 'kopakopa',\n",
       " 'kopakovira',\n",
       " 'kopato',\n",
       " 'kopii',\n",
       " 'kopiia',\n",
       " 'kopiipato',\n",
       " 'kopiipie',\n",
       " 'kopiito',\n",
       " 'kopikao',\n",
       " 'kopikopi',\n",
       " 'kopikopiara',\n",
       " 'kopipi',\n",
       " 'kopirovu',\n",
       " 'kopu',\n",
       " 'kopua',\n",
       " 'kopuasi',\n",
       " 'kopuasipie',\n",
       " 'kopuasito',\n",
       " 'kopuasitovira',\n",
       " 'kopuasivira',\n",
       " 'kopuisi',\n",
       " 'kopukopu',\n",
       " 'kopukopua',\n",
       " 'kopupa',\n",
       " 'kopupira',\n",
       " 'kopuro',\n",
       " 'kopuru',\n",
       " 'kopuvioro',\n",
       " 'kopuvira',\n",
       " 'kora',\n",
       " 'korapato',\n",
       " 'korara',\n",
       " 'koraraoko',\n",
       " 'korau',\n",
       " 'koraua',\n",
       " 'korauru',\n",
       " 'korauvira',\n",
       " 'korea',\n",
       " 'korekare',\n",
       " 'korere',\n",
       " 'korereto',\n",
       " 'kori',\n",
       " 'koria',\n",
       " 'koribori',\n",
       " 'korikori',\n",
       " 'korikoripava',\n",
       " 'korita',\n",
       " 'korita',\n",
       " 'koriteira',\n",
       " 'koro',\n",
       " 'kororo',\n",
       " 'kororo',\n",
       " 'kororoisivira',\n",
       " 'kororovi',\n",
       " 'kororovivira',\n",
       " 'kororu',\n",
       " 'kororupie',\n",
       " 'koroto',\n",
       " 'koroviri',\n",
       " 'korovo',\n",
       " 'koru',\n",
       " 'koruko',\n",
       " 'korukoru',\n",
       " 'korukorupato',\n",
       " 'koruo',\n",
       " 'koruoto',\n",
       " 'koruou',\n",
       " 'koruoua',\n",
       " 'koruovira',\n",
       " 'korupie',\n",
       " 'korupievira',\n",
       " 'kosi',\n",
       " 'kosikosi',\n",
       " 'kosikosi',\n",
       " 'kosipa',\n",
       " 'kosipato',\n",
       " 'kosipie',\n",
       " 'kosivago',\n",
       " 'kosiviro',\n",
       " 'koto',\n",
       " 'kotokoto',\n",
       " 'kotokotoara',\n",
       " 'kotopa',\n",
       " 'kotopa',\n",
       " 'kotovira',\n",
       " 'kotu',\n",
       " 'kotukotu',\n",
       " 'kotupiua',\n",
       " 'koturu',\n",
       " 'kou',\n",
       " 'kou',\n",
       " 'koue',\n",
       " 'koue',\n",
       " 'koui',\n",
       " 'koukou',\n",
       " 'koukouo',\n",
       " 'koukouo',\n",
       " 'kova',\n",
       " 'kovaaro',\n",
       " 'kovaeto',\n",
       " 'kovaii',\n",
       " 'kovakovara',\n",
       " 'kovapato',\n",
       " 'kovarato',\n",
       " 'kovarua',\n",
       " 'kovasi',\n",
       " 'kovata',\n",
       " 'kovatavira',\n",
       " 'kovato',\n",
       " 'kovauke',\n",
       " 'kovava',\n",
       " 'kove',\n",
       " 'kove',\n",
       " 'kovea',\n",
       " 'kovekove',\n",
       " 'koveoapa',\n",
       " 'koveva',\n",
       " 'kovia',\n",
       " 'kovikoro',\n",
       " 'kovire',\n",
       " 'kovirea',\n",
       " 'kovo',\n",
       " 'kovo',\n",
       " 'kovo',\n",
       " 'kovoa',\n",
       " 'kovokovo',\n",
       " 'kovokovo',\n",
       " 'kovokovo',\n",
       " 'kovokovoa',\n",
       " 'kovopaa',\n",
       " 'kovopato',\n",
       " 'kovopie',\n",
       " 'kovoruko',\n",
       " 'kovoto',\n",
       " 'kovovo',\n",
       " 'kovuaka',\n",
       " 'kovuaro',\n",
       " 'kovukovu',\n",
       " 'kovukovuto',\n",
       " 'kovukovuvira',\n",
       " 'kovuru',\n",
       " 'kovurui',\n",
       " 'kovuruko',\n",
       " 'kovurukovira',\n",
       " 'kovuruvira',\n",
       " 'kovuto',\n",
       " 'kuara',\n",
       " 'kue',\n",
       " 'kuea',\n",
       " 'kuga',\n",
       " 'kui',\n",
       " 'kuio',\n",
       " 'kuioi',\n",
       " 'kuiopesi',\n",
       " 'kuiopetu',\n",
       " 'kuito',\n",
       " 'kuka',\n",
       " 'kukara',\n",
       " 'kukauviro',\n",
       " 'kukiuki',\n",
       " 'kukiukia',\n",
       " 'kuku',\n",
       " 'kukua',\n",
       " 'kukuaua',\n",
       " 'kukue',\n",
       " 'kukue pute',\n",
       " 'kukuepaa',\n",
       " 'kukuku',\n",
       " 'kukupira',\n",
       " 'kukuriko',\n",
       " 'kukurikoto',\n",
       " 'kukusiri',\n",
       " 'kukutauvu',\n",
       " 'kukutu',\n",
       " 'kukuuku',\n",
       " 'kukuukua',\n",
       " 'kukuukupie',\n",
       " 'kukuuvua',\n",
       " 'kukuvai',\n",
       " 'kukuvaipaa',\n",
       " 'kukuvita',\n",
       " 'kukuvua',\n",
       " 'kuokuo',\n",
       " 'kupare',\n",
       " 'kupareto',\n",
       " 'kupekupe',\n",
       " 'kupekupepa',\n",
       " 'kupero',\n",
       " 'kuperoo',\n",
       " 'kuperovira',\n",
       " 'kupii',\n",
       " 'kupukupu',\n",
       " 'kurae',\n",
       " 'kurasia',\n",
       " 'kurasia',\n",
       " 'kurei',\n",
       " 'kuri',\n",
       " 'kuriato',\n",
       " 'kurikaakaakuto',\n",
       " 'kurikasi',\n",
       " 'kurikasivira',\n",
       " 'kurikoko',\n",
       " 'kurikuri',\n",
       " 'kuripaa',\n",
       " 'kuritava',\n",
       " 'kuroa',\n",
       " 'kuroea',\n",
       " 'kurokuro',\n",
       " 'kurokuroto',\n",
       " 'kuroo',\n",
       " 'kurooro',\n",
       " 'kurovira',\n",
       " 'kuru',\n",
       " 'kurue',\n",
       " 'kurupi',\n",
       " 'kururai',\n",
       " 'kururu',\n",
       " 'kurutu',\n",
       " 'kusi',\n",
       " 'kusii',\n",
       " 'kusike',\n",
       " 'kusito',\n",
       " 'kuu',\n",
       " 'kuu',\n",
       " 'kuukuuvu',\n",
       " 'kuukuuvuto',\n",
       " 'kuuoa',\n",
       " 'kuupie',\n",
       " 'kuurea',\n",
       " 'kuuri',\n",
       " 'kuuva',\n",
       " 'kuuvaki',\n",
       " 'kuuvakito',\n",
       " 'kuuvato',\n",
       " 'kuuvavira',\n",
       " 'kuuvu',\n",
       " 'kuuvuvira',\n",
       " 'kuva',\n",
       " 'kuvaku',\n",
       " 'kuvato',\n",
       " 'kuvau',\n",
       " 'kuvaupie',\n",
       " 'kuvauvira',\n",
       " 'kuvera',\n",
       " 'kuverava',\n",
       " 'kuvere',\n",
       " 'kuverea',\n",
       " 'kuvereto',\n",
       " 'kuverevira',\n",
       " 'kuvoro',\n",
       " 'kuvu',\n",
       " 'kuvuara',\n",
       " 'kuvukuvu',\n",
       " 'kuvukuvua',\n",
       " 'kuvupato',\n",
       " 'kuvuto']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lexeme.text.lower() for lexeme in lexicon.findall('record/lx')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the Toolbox data in XML format. The write() method of ElementTree expects a file object. We usually create one of these using Python's built-in open() function. In order to see the output displayed on the screen, we can use a special pre-defined file object called stdout [1] (standard output), defined in Python's sys module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<record>\n",
      "    <lx>kaa</lx>\n",
      "    <ps>N</ps>\n",
      "    <pt>MASC</pt>\n",
      "    <cl>isi</cl>\n",
      "    <ge>cooking banana</ge>\n",
      "    <tkp>banana bilong kukim</tkp>\n",
      "    <pt>itoo</pt>\n",
      "    <sf>FLORA</sf>\n",
      "    <dt>12/Aug/2005</dt>\n",
      "    <ex>Taeavi iria kaa isi kovopaueva kaparapasia.</ex>\n",
      "    <xp>Taeavi i bin planim gaden banana bilong kukim tasol long paia.</xp>\n",
      "    <xe>Taeavi planted banana in order to cook it.</xe>\n",
      "  </record>"
     ]
    }
   ],
   "source": [
    ">>> import sys\n",
    ">>> from nltk.util import elementtree_indent\n",
    ">>> from xml.etree.ElementTree import ElementTree\n",
    ">>> elementtree_indent(lexicon)\n",
    ">>> tree = ElementTree(lexicon[3])\n",
    ">>> tree.write(sys.stdout, encoding='unicode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5   Formatting Entries\n",
    "We can use the same idea we saw above to generate HTML tables instead of plain text. This would be useful for publishing a Toolbox lexicon on the web. It produces HTML elements <table>, <tr> (table row), and <td> (table data)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
