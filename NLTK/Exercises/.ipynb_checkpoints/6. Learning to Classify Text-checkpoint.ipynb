{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. \n",
    "Read up on one of the language technologies mentioned in this section, such as word sense disambiguation, semantic role labeling, question answering, machine translation, named entity detection. Find out what type and quantity of annotated data is required for developing such systems. Why do you think a large amount of data is required?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word sense disambiguation is imparitive in understanding a conveyed message. If someone states, \"They have my will\". We need to determine which sense \"will\" is. Is is a noun, that is the will determining where the person's belongings will go after death. A verb as in \"if there is a will, there is a way.\" \n",
    "\n",
    "This is dependent on labeled data, and SVMs are best for this. We need two types of data, a dictionary to specify the senses which are to be disambiguated and a corpus of language data to be disambiguated. Since disamiguation not only requires labels, definitions, and context it requires a fair amomunt of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. \n",
    "\n",
    "Using any of the three classifiers described in this chapter, and any features you can think of, build the best name gender classifier you can. Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev-test set, and the remaining 6900 words for the training set. Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are satisfied with your classifier, check its final performance on the test set. How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'k'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}\n",
    "gender_features('Shrek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])\n",
    "import random\n",
    "random.shuffle(labeled_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module nltk.classify.maxent in nltk.classify:\n",
      "\n",
      "NAME\n",
      "    nltk.classify.maxent\n",
      "\n",
      "DESCRIPTION\n",
      "    A classifier model based on maximum entropy modeling framework.  This\n",
      "    framework considers all of the probability distributions that are\n",
      "    empirically consistent with the training data; and chooses the\n",
      "    distribution with the highest entropy.  A probability distribution is\n",
      "    \"empirically consistent\" with a set of training data if its estimated\n",
      "    frequency with which a class and a feature vector value co-occur is\n",
      "    equal to the actual frequency in the data.\n",
      "    \n",
      "    Terminology: 'feature'\n",
      "    ======================\n",
      "    The term *feature* is usually used to refer to some property of an\n",
      "    unlabeled token.  For example, when performing word sense\n",
      "    disambiguation, we might define a ``'prevword'`` feature whose value is\n",
      "    the word preceding the target word.  However, in the context of\n",
      "    maxent modeling, the term *feature* is typically used to refer to a\n",
      "    property of a \"labeled\" token.  In order to prevent confusion, we\n",
      "    will introduce two distinct terms to disambiguate these two different\n",
      "    concepts:\n",
      "    \n",
      "      - An \"input-feature\" is a property of an unlabeled token.\n",
      "      - A \"joint-feature\" is a property of a labeled token.\n",
      "    \n",
      "    In the rest of the ``nltk.classify`` module, the term \"features\" is\n",
      "    used to refer to what we will call \"input-features\" in this module.\n",
      "    \n",
      "    In literature that describes and discusses maximum entropy models,\n",
      "    input-features are typically called \"contexts\", and joint-features\n",
      "    are simply referred to as \"features\".\n",
      "    \n",
      "    Converting Input-Features to Joint-Features\n",
      "    -------------------------------------------\n",
      "    In maximum entropy models, joint-features are required to have numeric\n",
      "    values.  Typically, each input-feature ``input_feat`` is mapped to a\n",
      "    set of joint-features of the form:\n",
      "    \n",
      "    |   joint_feat(token, label) = { 1 if input_feat(token) == feat_val\n",
      "    |                              {      and label == some_label\n",
      "    |                              {\n",
      "    |                              { 0 otherwise\n",
      "    \n",
      "    For all values of ``feat_val`` and ``some_label``.  This mapping is\n",
      "    performed by classes that implement the ``MaxentFeatureEncodingI``\n",
      "    interface.\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        MaxentFeatureEncodingI\n",
      "            BinaryMaxentFeatureEncoding\n",
      "                GISEncoding\n",
      "                TadmEventMaxentFeatureEncoding\n",
      "            FunctionBackedMaxentFeatureEncoding\n",
      "            TypedMaxentFeatureEncoding\n",
      "    nltk.classify.api.ClassifierI(builtins.object)\n",
      "        MaxentClassifier\n",
      "            TadmMaxentClassifier\n",
      "    \n",
      "    class BinaryMaxentFeatureEncoding(MaxentFeatureEncodingI)\n",
      "     |  BinaryMaxentFeatureEncoding(labels, mapping, unseen_features=False, alwayson_features=False)\n",
      "     |  \n",
      "     |  A feature encoding that generates vectors containing a binary\n",
      "     |  joint-features of the form:\n",
      "     |  \n",
      "     |  |  joint_feat(fs, l) = { 1 if (fs[fname] == fval) and (l == label)\n",
      "     |  |                      {\n",
      "     |  |                      { 0 otherwise\n",
      "     |  \n",
      "     |  Where ``fname`` is the name of an input-feature, ``fval`` is a value\n",
      "     |  for that input-feature, and ``label`` is a label.\n",
      "     |  \n",
      "     |  Typically, these features are constructed based on a training\n",
      "     |  corpus, using the ``train()`` method.  This method will create one\n",
      "     |  feature for each combination of ``fname``, ``fval``, and ``label``\n",
      "     |  that occurs at least once in the training corpus.\n",
      "     |  \n",
      "     |  The ``unseen_features`` parameter can be used to add \"unseen-value\n",
      "     |  features\", which are used whenever an input feature has a value\n",
      "     |  that was not encountered in the training corpus.  These features\n",
      "     |  have the form:\n",
      "     |  \n",
      "     |  |  joint_feat(fs, l) = { 1 if is_unseen(fname, fs[fname])\n",
      "     |  |                      {      and l == label\n",
      "     |  |                      {\n",
      "     |  |                      { 0 otherwise\n",
      "     |  \n",
      "     |  Where ``is_unseen(fname, fval)`` is true if the encoding does not\n",
      "     |  contain any joint features that are true when ``fs[fname]==fval``.\n",
      "     |  \n",
      "     |  The ``alwayson_features`` parameter can be used to add \"always-on\n",
      "     |  features\", which have the form::\n",
      "     |  \n",
      "     |  |  joint_feat(fs, l) = { 1 if (l == label)\n",
      "     |  |                      {\n",
      "     |  |                      { 0 otherwise\n",
      "     |  \n",
      "     |  These always-on features allow the maxent model to directly model\n",
      "     |  the prior probabilities of each label.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BinaryMaxentFeatureEncoding\n",
      "     |      MaxentFeatureEncodingI\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, labels, mapping, unseen_features=False, alwayson_features=False)\n",
      "     |      :param labels: A list of the \"known labels\" for this encoding.\n",
      "     |      \n",
      "     |      :param mapping: A dictionary mapping from ``(fname,fval,label)``\n",
      "     |          tuples to corresponding joint-feature indexes.  These\n",
      "     |          indexes must be the set of integers from 0...len(mapping).\n",
      "     |          If ``mapping[fname,fval,label]=id``, then\n",
      "     |          ``self.encode(..., fname:fval, ..., label)[id]`` is 1;\n",
      "     |          otherwise, it is 0.\n",
      "     |      \n",
      "     |      :param unseen_features: If true, then include unseen value\n",
      "     |         features in the generated joint-feature vectors.\n",
      "     |      \n",
      "     |      :param alwayson_features: If true, then include always-on\n",
      "     |         features in the generated joint-feature vectors.\n",
      "     |  \n",
      "     |  describe(self, f_id)\n",
      "     |      :return: A string describing the value of the joint-feature\n",
      "     |          whose index in the generated feature vectors is ``fid``.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  encode(self, featureset, label)\n",
      "     |      Given a (featureset, label) pair, return the corresponding\n",
      "     |      vector of joint-feature values.  This vector is represented as\n",
      "     |      a list of ``(index, value)`` tuples, specifying the value of\n",
      "     |      each non-zero joint-feature.\n",
      "     |      \n",
      "     |      :type featureset: dict\n",
      "     |      :rtype: list(tuple(int, int))\n",
      "     |  \n",
      "     |  labels(self)\n",
      "     |      :return: A list of the \"known labels\" -- i.e., all labels\n",
      "     |          ``l`` such that ``self.encode(fs,l)`` can be a nonzero\n",
      "     |          joint-feature vector for some value of ``fs``.\n",
      "     |      :rtype: list\n",
      "     |  \n",
      "     |  length(self)\n",
      "     |      :return: The size of the fixed-length joint-feature vectors\n",
      "     |          that are generated by this encoding.\n",
      "     |      :rtype: int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  train(train_toks, count_cutoff=0, labels=None, **options) from builtins.type\n",
      "     |      Construct and return new feature encoding, based on a given\n",
      "     |      training corpus ``train_toks``.  See the class description\n",
      "     |      ``BinaryMaxentFeatureEncoding`` for a description of the\n",
      "     |      joint-features that will be included in this encoding.\n",
      "     |      \n",
      "     |      :type train_toks: list(tuple(dict, str))\n",
      "     |      :param train_toks: Training data, represented as a list of\n",
      "     |          pairs, the first member of which is a feature dictionary,\n",
      "     |          and the second of which is a classification label.\n",
      "     |      \n",
      "     |      :type count_cutoff: int\n",
      "     |      :param count_cutoff: A cutoff value that is used to discard\n",
      "     |          rare joint-features.  If a joint-feature's value is 1\n",
      "     |          fewer than ``count_cutoff`` times in the training corpus,\n",
      "     |          then that joint-feature is not included in the generated\n",
      "     |          encoding.\n",
      "     |      \n",
      "     |      :type labels: list\n",
      "     |      :param labels: A list of labels that should be used by the\n",
      "     |          classifier.  If not specified, then the set of labels\n",
      "     |          attested in ``train_toks`` will be used.\n",
      "     |      \n",
      "     |      :param options: Extra parameters for the constructor, such as\n",
      "     |          ``unseen_features`` and ``alwayson_features``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from MaxentFeatureEncodingI:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    ConditionalExponentialClassifier = class MaxentClassifier(nltk.classify.api.ClassifierI)\n",
      "     |  ConditionalExponentialClassifier(encoding, weights, logarithmic=True)\n",
      "     |  \n",
      "     |  A maximum entropy classifier (also known as a \"conditional\n",
      "     |  exponential classifier\").  This classifier is parameterized by a\n",
      "     |  set of \"weights\", which are used to combine the joint-features\n",
      "     |  that are generated from a featureset by an \"encoding\".  In\n",
      "     |  particular, the encoding maps each ``(featureset, label)`` pair to\n",
      "     |  a vector.  The probability of each label is then computed using\n",
      "     |  the following equation::\n",
      "     |  \n",
      "     |                              dotprod(weights, encode(fs,label))\n",
      "     |    prob(fs|label) = ---------------------------------------------------\n",
      "     |                     sum(dotprod(weights, encode(fs,l)) for l in labels)\n",
      "     |  \n",
      "     |  Where ``dotprod`` is the dot product::\n",
      "     |  \n",
      "     |    dotprod(a,b) = sum(x*y for (x,y) in zip(a,b))\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MaxentClassifier\n",
      "     |      nltk.classify.api.ClassifierI\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, encoding, weights, logarithmic=True)\n",
      "     |      Construct a new maxent classifier model.  Typically, new\n",
      "     |      classifier models are created using the ``train()`` method.\n",
      "     |      \n",
      "     |      :type encoding: MaxentFeatureEncodingI\n",
      "     |      :param encoding: An encoding that is used to convert the\n",
      "     |          featuresets that are given to the ``classify`` method into\n",
      "     |          joint-feature vectors, which are used by the maxent\n",
      "     |          classifier model.\n",
      "     |      \n",
      "     |      :type weights: list of float\n",
      "     |      :param weights:  The feature weight vector for this classifier.\n",
      "     |      \n",
      "     |      :type logarithmic: bool\n",
      "     |      :param logarithmic: If false, then use non-logarithmic weights.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  classify(self, featureset)\n",
      "     |      :return: the most appropriate label for the given featureset.\n",
      "     |      :rtype: label\n",
      "     |  \n",
      "     |  explain(self, featureset, columns=4)\n",
      "     |      Print a table showing the effect of each of the features in\n",
      "     |      the given feature set, and how they combine to determine the\n",
      "     |      probabilities of each label for that featureset.\n",
      "     |  \n",
      "     |  labels(self)\n",
      "     |      :return: the list of category labels used by this classifier.\n",
      "     |      :rtype: list of (immutable)\n",
      "     |  \n",
      "     |  most_informative_features(self, n=10)\n",
      "     |      Generates the ranked list of informative features from most to least.\n",
      "     |  \n",
      "     |  prob_classify(self, featureset)\n",
      "     |      :return: a probability distribution over labels for the given\n",
      "     |          featureset.\n",
      "     |      :rtype: ProbDistI\n",
      "     |  \n",
      "     |  set_weights(self, new_weights)\n",
      "     |      Set the feature weight vector for this classifier.\n",
      "     |      :param new_weights: The new feature weight vector.\n",
      "     |      :type new_weights: list of float\n",
      "     |  \n",
      "     |  show_most_informative_features(self, n=10, show='all')\n",
      "     |      :param show: all, neg, or pos (for negative-only or positive-only)\n",
      "     |      :type show: str\n",
      "     |      :param n: The no. of top features\n",
      "     |      :type n: int\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  weights(self)\n",
      "     |      :return: The feature weight vector for this classifier.\n",
      "     |      :rtype: list of float\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  train(train_toks, algorithm=None, trace=3, encoding=None, labels=None, gaussian_prior_sigma=0, **cutoffs) from builtins.type\n",
      "     |      Train a new maxent classifier based on the given corpus of\n",
      "     |      training samples.  This classifier will have its weights\n",
      "     |      chosen to maximize entropy while remaining empirically\n",
      "     |      consistent with the training corpus.\n",
      "     |      \n",
      "     |      :rtype: MaxentClassifier\n",
      "     |      :return: The new maxent classifier\n",
      "     |      \n",
      "     |      :type train_toks: list\n",
      "     |      :param train_toks: Training data, represented as a list of\n",
      "     |          pairs, the first member of which is a featureset,\n",
      "     |          and the second of which is a classification label.\n",
      "     |      \n",
      "     |      :type algorithm: str\n",
      "     |      :param algorithm: A case-insensitive string, specifying which\n",
      "     |          algorithm should be used to train the classifier.  The\n",
      "     |          following algorithms are currently available.\n",
      "     |      \n",
      "     |          - Iterative Scaling Methods: Generalized Iterative Scaling (``'GIS'``),\n",
      "     |            Improved Iterative Scaling (``'IIS'``)\n",
      "     |          - External Libraries (requiring megam):\n",
      "     |            LM-BFGS algorithm, with training performed by Megam (``'megam'``)\n",
      "     |      \n",
      "     |          The default algorithm is ``'IIS'``.\n",
      "     |      \n",
      "     |      :type trace: int\n",
      "     |      :param trace: The level of diagnostic tracing output to produce.\n",
      "     |          Higher values produce more verbose output.\n",
      "     |      :type encoding: MaxentFeatureEncodingI\n",
      "     |      :param encoding: A feature encoding, used to convert featuresets\n",
      "     |          into feature vectors.  If none is specified, then a\n",
      "     |          ``BinaryMaxentFeatureEncoding`` will be built based on the\n",
      "     |          features that are attested in the training corpus.\n",
      "     |      :type labels: list(str)\n",
      "     |      :param labels: The set of possible labels.  If none is given, then\n",
      "     |          the set of all labels attested in the training data will be\n",
      "     |          used instead.\n",
      "     |      :param gaussian_prior_sigma: The sigma value for a gaussian\n",
      "     |          prior on model weights.  Currently, this is supported by\n",
      "     |          ``megam``. For other algorithms, its value is ignored.\n",
      "     |      :param cutoffs: Arguments specifying various conditions under\n",
      "     |          which the training should be halted.  (Some of the cutoff\n",
      "     |          conditions are not supported by some algorithms.)\n",
      "     |      \n",
      "     |          - ``max_iter=v``: Terminate after ``v`` iterations.\n",
      "     |          - ``min_ll=v``: Terminate after the negative average\n",
      "     |            log-likelihood drops under ``v``.\n",
      "     |          - ``min_lldelta=v``: Terminate if a single iteration improves\n",
      "     |            log likelihood by less than ``v``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  ALGORITHMS = ['GIS', 'IIS', 'MEGAM', 'TADM']\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.classify.api.ClassifierI:\n",
      "     |  \n",
      "     |  classify_many(self, featuresets)\n",
      "     |      Apply ``self.classify()`` to each element of ``featuresets``.  I.e.:\n",
      "     |      \n",
      "     |          return [self.classify(fs) for fs in featuresets]\n",
      "     |      \n",
      "     |      :rtype: list(label)\n",
      "     |  \n",
      "     |  prob_classify_many(self, featuresets)\n",
      "     |      Apply ``self.prob_classify()`` to each element of ``featuresets``.  I.e.:\n",
      "     |      \n",
      "     |          return [self.prob_classify(fs) for fs in featuresets]\n",
      "     |      \n",
      "     |      :rtype: list(ProbDistI)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.classify.api.ClassifierI:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class FunctionBackedMaxentFeatureEncoding(MaxentFeatureEncodingI)\n",
      "     |  FunctionBackedMaxentFeatureEncoding(func, length, labels)\n",
      "     |  \n",
      "     |  A feature encoding that calls a user-supplied function to map a\n",
      "     |  given featureset/label pair to a sparse joint-feature vector.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FunctionBackedMaxentFeatureEncoding\n",
      "     |      MaxentFeatureEncodingI\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, func, length, labels)\n",
      "     |      Construct a new feature encoding based on the given function.\n",
      "     |      \n",
      "     |      :type func: (callable)\n",
      "     |      :param func: A function that takes two arguments, a featureset\n",
      "     |           and a label, and returns the sparse joint feature vector\n",
      "     |           that encodes them::\n",
      "     |      \n",
      "     |               func(featureset, label) -> feature_vector\n",
      "     |      \n",
      "     |           This sparse joint feature vector (``feature_vector``) is a\n",
      "     |           list of ``(index,value)`` tuples.\n",
      "     |      \n",
      "     |      :type length: int\n",
      "     |      :param length: The size of the fixed-length joint-feature\n",
      "     |          vectors that are generated by this encoding.\n",
      "     |      \n",
      "     |      :type labels: list\n",
      "     |      :param labels: A list of the \"known labels\" for this\n",
      "     |          encoding -- i.e., all labels ``l`` such that\n",
      "     |          ``self.encode(fs,l)`` can be a nonzero joint-feature vector\n",
      "     |          for some value of ``fs``.\n",
      "     |  \n",
      "     |  describe(self, fid)\n",
      "     |      :return: A string describing the value of the joint-feature\n",
      "     |          whose index in the generated feature vectors is ``fid``.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  encode(self, featureset, label)\n",
      "     |      Given a (featureset, label) pair, return the corresponding\n",
      "     |      vector of joint-feature values.  This vector is represented as\n",
      "     |      a list of ``(index, value)`` tuples, specifying the value of\n",
      "     |      each non-zero joint-feature.\n",
      "     |      \n",
      "     |      :type featureset: dict\n",
      "     |      :rtype: list(tuple(int, int))\n",
      "     |  \n",
      "     |  labels(self)\n",
      "     |      :return: A list of the \"known labels\" -- i.e., all labels\n",
      "     |          ``l`` such that ``self.encode(fs,l)`` can be a nonzero\n",
      "     |          joint-feature vector for some value of ``fs``.\n",
      "     |      :rtype: list\n",
      "     |  \n",
      "     |  length(self)\n",
      "     |      :return: The size of the fixed-length joint-feature vectors\n",
      "     |          that are generated by this encoding.\n",
      "     |      :rtype: int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from MaxentFeatureEncodingI:\n",
      "     |  \n",
      "     |  train(cls, train_toks)\n",
      "     |      Construct and return new feature encoding, based on a given\n",
      "     |      training corpus ``train_toks``.\n",
      "     |      \n",
      "     |      :type train_toks: list(tuple(dict, str))\n",
      "     |      :param train_toks: Training data, represented as a list of\n",
      "     |          pairs, the first member of which is a feature dictionary,\n",
      "     |          and the second of which is a classification label.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from MaxentFeatureEncodingI:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class GISEncoding(BinaryMaxentFeatureEncoding)\n",
      "     |  GISEncoding(labels, mapping, unseen_features=False, alwayson_features=False, C=None)\n",
      "     |  \n",
      "     |  A binary feature encoding which adds one new joint-feature to the\n",
      "     |  joint-features defined by ``BinaryMaxentFeatureEncoding``: a\n",
      "     |  correction feature, whose value is chosen to ensure that the\n",
      "     |  sparse vector always sums to a constant non-negative number.  This\n",
      "     |  new feature is used to ensure two preconditions for the GIS\n",
      "     |  training algorithm:\n",
      "     |  \n",
      "     |    - At least one feature vector index must be nonzero for every\n",
      "     |      token.\n",
      "     |    - The feature vector must sum to a constant non-negative number\n",
      "     |      for every token.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GISEncoding\n",
      "     |      BinaryMaxentFeatureEncoding\n",
      "     |      MaxentFeatureEncodingI\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, labels, mapping, unseen_features=False, alwayson_features=False, C=None)\n",
      "     |      :param C: The correction constant.  The value of the correction\n",
      "     |          feature is based on this value.  In particular, its value is\n",
      "     |          ``C - sum([v for (f,v) in encoding])``.\n",
      "     |      :seealso: ``BinaryMaxentFeatureEncoding.__init__``\n",
      "     |  \n",
      "     |  describe(self, f_id)\n",
      "     |      :return: A string describing the value of the joint-feature\n",
      "     |          whose index in the generated feature vectors is ``fid``.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  encode(self, featureset, label)\n",
      "     |      Given a (featureset, label) pair, return the corresponding\n",
      "     |      vector of joint-feature values.  This vector is represented as\n",
      "     |      a list of ``(index, value)`` tuples, specifying the value of\n",
      "     |      each non-zero joint-feature.\n",
      "     |      \n",
      "     |      :type featureset: dict\n",
      "     |      :rtype: list(tuple(int, int))\n",
      "     |  \n",
      "     |  length(self)\n",
      "     |      :return: The size of the fixed-length joint-feature vectors\n",
      "     |          that are generated by this encoding.\n",
      "     |      :rtype: int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  C\n",
      "     |      The non-negative constant that all encoded feature vectors\n",
      "     |      will sum to.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BinaryMaxentFeatureEncoding:\n",
      "     |  \n",
      "     |  labels(self)\n",
      "     |      :return: A list of the \"known labels\" -- i.e., all labels\n",
      "     |          ``l`` such that ``self.encode(fs,l)`` can be a nonzero\n",
      "     |          joint-feature vector for some value of ``fs``.\n",
      "     |      :rtype: list\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from BinaryMaxentFeatureEncoding:\n",
      "     |  \n",
      "     |  train(train_toks, count_cutoff=0, labels=None, **options) from builtins.type\n",
      "     |      Construct and return new feature encoding, based on a given\n",
      "     |      training corpus ``train_toks``.  See the class description\n",
      "     |      ``BinaryMaxentFeatureEncoding`` for a description of the\n",
      "     |      joint-features that will be included in this encoding.\n",
      "     |      \n",
      "     |      :type train_toks: list(tuple(dict, str))\n",
      "     |      :param train_toks: Training data, represented as a list of\n",
      "     |          pairs, the first member of which is a feature dictionary,\n",
      "     |          and the second of which is a classification label.\n",
      "     |      \n",
      "     |      :type count_cutoff: int\n",
      "     |      :param count_cutoff: A cutoff value that is used to discard\n",
      "     |          rare joint-features.  If a joint-feature's value is 1\n",
      "     |          fewer than ``count_cutoff`` times in the training corpus,\n",
      "     |          then that joint-feature is not included in the generated\n",
      "     |          encoding.\n",
      "     |      \n",
      "     |      :type labels: list\n",
      "     |      :param labels: A list of labels that should be used by the\n",
      "     |          classifier.  If not specified, then the set of labels\n",
      "     |          attested in ``train_toks`` will be used.\n",
      "     |      \n",
      "     |      :param options: Extra parameters for the constructor, such as\n",
      "     |          ``unseen_features`` and ``alwayson_features``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from MaxentFeatureEncodingI:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MaxentClassifier(nltk.classify.api.ClassifierI)\n",
      "     |  MaxentClassifier(encoding, weights, logarithmic=True)\n",
      "     |  \n",
      "     |  A maximum entropy classifier (also known as a \"conditional\n",
      "     |  exponential classifier\").  This classifier is parameterized by a\n",
      "     |  set of \"weights\", which are used to combine the joint-features\n",
      "     |  that are generated from a featureset by an \"encoding\".  In\n",
      "     |  particular, the encoding maps each ``(featureset, label)`` pair to\n",
      "     |  a vector.  The probability of each label is then computed using\n",
      "     |  the following equation::\n",
      "     |  \n",
      "     |                              dotprod(weights, encode(fs,label))\n",
      "     |    prob(fs|label) = ---------------------------------------------------\n",
      "     |                     sum(dotprod(weights, encode(fs,l)) for l in labels)\n",
      "     |  \n",
      "     |  Where ``dotprod`` is the dot product::\n",
      "     |  \n",
      "     |    dotprod(a,b) = sum(x*y for (x,y) in zip(a,b))\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MaxentClassifier\n",
      "     |      nltk.classify.api.ClassifierI\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, encoding, weights, logarithmic=True)\n",
      "     |      Construct a new maxent classifier model.  Typically, new\n",
      "     |      classifier models are created using the ``train()`` method.\n",
      "     |      \n",
      "     |      :type encoding: MaxentFeatureEncodingI\n",
      "     |      :param encoding: An encoding that is used to convert the\n",
      "     |          featuresets that are given to the ``classify`` method into\n",
      "     |          joint-feature vectors, which are used by the maxent\n",
      "     |          classifier model.\n",
      "     |      \n",
      "     |      :type weights: list of float\n",
      "     |      :param weights:  The feature weight vector for this classifier.\n",
      "     |      \n",
      "     |      :type logarithmic: bool\n",
      "     |      :param logarithmic: If false, then use non-logarithmic weights.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  classify(self, featureset)\n",
      "     |      :return: the most appropriate label for the given featureset.\n",
      "     |      :rtype: label\n",
      "     |  \n",
      "     |  explain(self, featureset, columns=4)\n",
      "     |      Print a table showing the effect of each of the features in\n",
      "     |      the given feature set, and how they combine to determine the\n",
      "     |      probabilities of each label for that featureset.\n",
      "     |  \n",
      "     |  labels(self)\n",
      "     |      :return: the list of category labels used by this classifier.\n",
      "     |      :rtype: list of (immutable)\n",
      "     |  \n",
      "     |  most_informative_features(self, n=10)\n",
      "     |      Generates the ranked list of informative features from most to least.\n",
      "     |  \n",
      "     |  prob_classify(self, featureset)\n",
      "     |      :return: a probability distribution over labels for the given\n",
      "     |          featureset.\n",
      "     |      :rtype: ProbDistI\n",
      "     |  \n",
      "     |  set_weights(self, new_weights)\n",
      "     |      Set the feature weight vector for this classifier.\n",
      "     |      :param new_weights: The new feature weight vector.\n",
      "     |      :type new_weights: list of float\n",
      "     |  \n",
      "     |  show_most_informative_features(self, n=10, show='all')\n",
      "     |      :param show: all, neg, or pos (for negative-only or positive-only)\n",
      "     |      :type show: str\n",
      "     |      :param n: The no. of top features\n",
      "     |      :type n: int\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  weights(self)\n",
      "     |      :return: The feature weight vector for this classifier.\n",
      "     |      :rtype: list of float\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  train(train_toks, algorithm=None, trace=3, encoding=None, labels=None, gaussian_prior_sigma=0, **cutoffs) from builtins.type\n",
      "     |      Train a new maxent classifier based on the given corpus of\n",
      "     |      training samples.  This classifier will have its weights\n",
      "     |      chosen to maximize entropy while remaining empirically\n",
      "     |      consistent with the training corpus.\n",
      "     |      \n",
      "     |      :rtype: MaxentClassifier\n",
      "     |      :return: The new maxent classifier\n",
      "     |      \n",
      "     |      :type train_toks: list\n",
      "     |      :param train_toks: Training data, represented as a list of\n",
      "     |          pairs, the first member of which is a featureset,\n",
      "     |          and the second of which is a classification label.\n",
      "     |      \n",
      "     |      :type algorithm: str\n",
      "     |      :param algorithm: A case-insensitive string, specifying which\n",
      "     |          algorithm should be used to train the classifier.  The\n",
      "     |          following algorithms are currently available.\n",
      "     |      \n",
      "     |          - Iterative Scaling Methods: Generalized Iterative Scaling (``'GIS'``),\n",
      "     |            Improved Iterative Scaling (``'IIS'``)\n",
      "     |          - External Libraries (requiring megam):\n",
      "     |            LM-BFGS algorithm, with training performed by Megam (``'megam'``)\n",
      "     |      \n",
      "     |          The default algorithm is ``'IIS'``.\n",
      "     |      \n",
      "     |      :type trace: int\n",
      "     |      :param trace: The level of diagnostic tracing output to produce.\n",
      "     |          Higher values produce more verbose output.\n",
      "     |      :type encoding: MaxentFeatureEncodingI\n",
      "     |      :param encoding: A feature encoding, used to convert featuresets\n",
      "     |          into feature vectors.  If none is specified, then a\n",
      "     |          ``BinaryMaxentFeatureEncoding`` will be built based on the\n",
      "     |          features that are attested in the training corpus.\n",
      "     |      :type labels: list(str)\n",
      "     |      :param labels: The set of possible labels.  If none is given, then\n",
      "     |          the set of all labels attested in the training data will be\n",
      "     |          used instead.\n",
      "     |      :param gaussian_prior_sigma: The sigma value for a gaussian\n",
      "     |          prior on model weights.  Currently, this is supported by\n",
      "     |          ``megam``. For other algorithms, its value is ignored.\n",
      "     |      :param cutoffs: Arguments specifying various conditions under\n",
      "     |          which the training should be halted.  (Some of the cutoff\n",
      "     |          conditions are not supported by some algorithms.)\n",
      "     |      \n",
      "     |          - ``max_iter=v``: Terminate after ``v`` iterations.\n",
      "     |          - ``min_ll=v``: Terminate after the negative average\n",
      "     |            log-likelihood drops under ``v``.\n",
      "     |          - ``min_lldelta=v``: Terminate if a single iteration improves\n",
      "     |            log likelihood by less than ``v``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  ALGORITHMS = ['GIS', 'IIS', 'MEGAM', 'TADM']\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.classify.api.ClassifierI:\n",
      "     |  \n",
      "     |  classify_many(self, featuresets)\n",
      "     |      Apply ``self.classify()`` to each element of ``featuresets``.  I.e.:\n",
      "     |      \n",
      "     |          return [self.classify(fs) for fs in featuresets]\n",
      "     |      \n",
      "     |      :rtype: list(label)\n",
      "     |  \n",
      "     |  prob_classify_many(self, featuresets)\n",
      "     |      Apply ``self.prob_classify()`` to each element of ``featuresets``.  I.e.:\n",
      "     |      \n",
      "     |          return [self.prob_classify(fs) for fs in featuresets]\n",
      "     |      \n",
      "     |      :rtype: list(ProbDistI)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.classify.api.ClassifierI:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MaxentFeatureEncodingI(builtins.object)\n",
      "     |  A mapping that converts a set of input-feature values to a vector\n",
      "     |  of joint-feature values, given a label.  This conversion is\n",
      "     |  necessary to translate featuresets into a format that can be used\n",
      "     |  by maximum entropy models.\n",
      "     |  \n",
      "     |  The set of joint-features used by a given encoding is fixed, and\n",
      "     |  each index in the generated joint-feature vectors corresponds to a\n",
      "     |  single joint-feature.  The length of the generated joint-feature\n",
      "     |  vectors is therefore constant (for a given encoding).\n",
      "     |  \n",
      "     |  Because the joint-feature vectors generated by\n",
      "     |  ``MaxentFeatureEncodingI`` are typically very sparse, they are\n",
      "     |  represented as a list of ``(index, value)`` tuples, specifying the\n",
      "     |  value of each non-zero joint-feature.\n",
      "     |  \n",
      "     |  Feature encodings are generally created using the ``train()``\n",
      "     |  method, which generates an appropriate encoding based on the\n",
      "     |  input-feature values and labels that are present in a given\n",
      "     |  corpus.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  describe(self, fid)\n",
      "     |      :return: A string describing the value of the joint-feature\n",
      "     |          whose index in the generated feature vectors is ``fid``.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  encode(self, featureset, label)\n",
      "     |      Given a (featureset, label) pair, return the corresponding\n",
      "     |      vector of joint-feature values.  This vector is represented as\n",
      "     |      a list of ``(index, value)`` tuples, specifying the value of\n",
      "     |      each non-zero joint-feature.\n",
      "     |      \n",
      "     |      :type featureset: dict\n",
      "     |      :rtype: list(tuple(int, int))\n",
      "     |  \n",
      "     |  labels(self)\n",
      "     |      :return: A list of the \"known labels\" -- i.e., all labels\n",
      "     |          ``l`` such that ``self.encode(fs,l)`` can be a nonzero\n",
      "     |          joint-feature vector for some value of ``fs``.\n",
      "     |      :rtype: list\n",
      "     |  \n",
      "     |  length(self)\n",
      "     |      :return: The size of the fixed-length joint-feature vectors\n",
      "     |          that are generated by this encoding.\n",
      "     |      :rtype: int\n",
      "     |  \n",
      "     |  train(cls, train_toks)\n",
      "     |      Construct and return new feature encoding, based on a given\n",
      "     |      training corpus ``train_toks``.\n",
      "     |      \n",
      "     |      :type train_toks: list(tuple(dict, str))\n",
      "     |      :param train_toks: Training data, represented as a list of\n",
      "     |          pairs, the first member of which is a feature dictionary,\n",
      "     |          and the second of which is a classification label.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TadmEventMaxentFeatureEncoding(BinaryMaxentFeatureEncoding)\n",
      "     |  TadmEventMaxentFeatureEncoding(labels, mapping, unseen_features=False, alwayson_features=False)\n",
      "     |  \n",
      "     |  A feature encoding that generates vectors containing a binary\n",
      "     |  joint-features of the form:\n",
      "     |  \n",
      "     |  |  joint_feat(fs, l) = { 1 if (fs[fname] == fval) and (l == label)\n",
      "     |  |                      {\n",
      "     |  |                      { 0 otherwise\n",
      "     |  \n",
      "     |  Where ``fname`` is the name of an input-feature, ``fval`` is a value\n",
      "     |  for that input-feature, and ``label`` is a label.\n",
      "     |  \n",
      "     |  Typically, these features are constructed based on a training\n",
      "     |  corpus, using the ``train()`` method.  This method will create one\n",
      "     |  feature for each combination of ``fname``, ``fval``, and ``label``\n",
      "     |  that occurs at least once in the training corpus.\n",
      "     |  \n",
      "     |  The ``unseen_features`` parameter can be used to add \"unseen-value\n",
      "     |  features\", which are used whenever an input feature has a value\n",
      "     |  that was not encountered in the training corpus.  These features\n",
      "     |  have the form:\n",
      "     |  \n",
      "     |  |  joint_feat(fs, l) = { 1 if is_unseen(fname, fs[fname])\n",
      "     |  |                      {      and l == label\n",
      "     |  |                      {\n",
      "     |  |                      { 0 otherwise\n",
      "     |  \n",
      "     |  Where ``is_unseen(fname, fval)`` is true if the encoding does not\n",
      "     |  contain any joint features that are true when ``fs[fname]==fval``.\n",
      "     |  \n",
      "     |  The ``alwayson_features`` parameter can be used to add \"always-on\n",
      "     |  features\", which have the form::\n",
      "     |  \n",
      "     |  |  joint_feat(fs, l) = { 1 if (l == label)\n",
      "     |  |                      {\n",
      "     |  |                      { 0 otherwise\n",
      "     |  \n",
      "     |  These always-on features allow the maxent model to directly model\n",
      "     |  the prior probabilities of each label.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TadmEventMaxentFeatureEncoding\n",
      "     |      BinaryMaxentFeatureEncoding\n",
      "     |      MaxentFeatureEncodingI\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, labels, mapping, unseen_features=False, alwayson_features=False)\n",
      "     |      :param labels: A list of the \"known labels\" for this encoding.\n",
      "     |      \n",
      "     |      :param mapping: A dictionary mapping from ``(fname,fval,label)``\n",
      "     |          tuples to corresponding joint-feature indexes.  These\n",
      "     |          indexes must be the set of integers from 0...len(mapping).\n",
      "     |          If ``mapping[fname,fval,label]=id``, then\n",
      "     |          ``self.encode(..., fname:fval, ..., label)[id]`` is 1;\n",
      "     |          otherwise, it is 0.\n",
      "     |      \n",
      "     |      :param unseen_features: If true, then include unseen value\n",
      "     |         features in the generated joint-feature vectors.\n",
      "     |      \n",
      "     |      :param alwayson_features: If true, then include always-on\n",
      "     |         features in the generated joint-feature vectors.\n",
      "     |  \n",
      "     |  describe(self, fid)\n",
      "     |      :return: A string describing the value of the joint-feature\n",
      "     |          whose index in the generated feature vectors is ``fid``.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  encode(self, featureset, label)\n",
      "     |      Given a (featureset, label) pair, return the corresponding\n",
      "     |      vector of joint-feature values.  This vector is represented as\n",
      "     |      a list of ``(index, value)`` tuples, specifying the value of\n",
      "     |      each non-zero joint-feature.\n",
      "     |      \n",
      "     |      :type featureset: dict\n",
      "     |      :rtype: list(tuple(int, int))\n",
      "     |  \n",
      "     |  labels(self)\n",
      "     |      :return: A list of the \"known labels\" -- i.e., all labels\n",
      "     |          ``l`` such that ``self.encode(fs,l)`` can be a nonzero\n",
      "     |          joint-feature vector for some value of ``fs``.\n",
      "     |      :rtype: list\n",
      "     |  \n",
      "     |  length(self)\n",
      "     |      :return: The size of the fixed-length joint-feature vectors\n",
      "     |          that are generated by this encoding.\n",
      "     |      :rtype: int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  train(train_toks, count_cutoff=0, labels=None, **options) from builtins.type\n",
      "     |      Construct and return new feature encoding, based on a given\n",
      "     |      training corpus ``train_toks``.  See the class description\n",
      "     |      ``BinaryMaxentFeatureEncoding`` for a description of the\n",
      "     |      joint-features that will be included in this encoding.\n",
      "     |      \n",
      "     |      :type train_toks: list(tuple(dict, str))\n",
      "     |      :param train_toks: Training data, represented as a list of\n",
      "     |          pairs, the first member of which is a feature dictionary,\n",
      "     |          and the second of which is a classification label.\n",
      "     |      \n",
      "     |      :type count_cutoff: int\n",
      "     |      :param count_cutoff: A cutoff value that is used to discard\n",
      "     |          rare joint-features.  If a joint-feature's value is 1\n",
      "     |          fewer than ``count_cutoff`` times in the training corpus,\n",
      "     |          then that joint-feature is not included in the generated\n",
      "     |          encoding.\n",
      "     |      \n",
      "     |      :type labels: list\n",
      "     |      :param labels: A list of labels that should be used by the\n",
      "     |          classifier.  If not specified, then the set of labels\n",
      "     |          attested in ``train_toks`` will be used.\n",
      "     |      \n",
      "     |      :param options: Extra parameters for the constructor, such as\n",
      "     |          ``unseen_features`` and ``alwayson_features``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from MaxentFeatureEncodingI:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TadmMaxentClassifier(MaxentClassifier)\n",
      "     |  TadmMaxentClassifier(encoding, weights, logarithmic=True)\n",
      "     |  \n",
      "     |  A maximum entropy classifier (also known as a \"conditional\n",
      "     |  exponential classifier\").  This classifier is parameterized by a\n",
      "     |  set of \"weights\", which are used to combine the joint-features\n",
      "     |  that are generated from a featureset by an \"encoding\".  In\n",
      "     |  particular, the encoding maps each ``(featureset, label)`` pair to\n",
      "     |  a vector.  The probability of each label is then computed using\n",
      "     |  the following equation::\n",
      "     |  \n",
      "     |                              dotprod(weights, encode(fs,label))\n",
      "     |    prob(fs|label) = ---------------------------------------------------\n",
      "     |                     sum(dotprod(weights, encode(fs,l)) for l in labels)\n",
      "     |  \n",
      "     |  Where ``dotprod`` is the dot product::\n",
      "     |  \n",
      "     |    dotprod(a,b) = sum(x*y for (x,y) in zip(a,b))\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TadmMaxentClassifier\n",
      "     |      MaxentClassifier\n",
      "     |      nltk.classify.api.ClassifierI\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  train(train_toks, **kwargs) from builtins.type\n",
      "     |      Train a new maxent classifier based on the given corpus of\n",
      "     |      training samples.  This classifier will have its weights\n",
      "     |      chosen to maximize entropy while remaining empirically\n",
      "     |      consistent with the training corpus.\n",
      "     |      \n",
      "     |      :rtype: MaxentClassifier\n",
      "     |      :return: The new maxent classifier\n",
      "     |      \n",
      "     |      :type train_toks: list\n",
      "     |      :param train_toks: Training data, represented as a list of\n",
      "     |          pairs, the first member of which is a featureset,\n",
      "     |          and the second of which is a classification label.\n",
      "     |      \n",
      "     |      :type algorithm: str\n",
      "     |      :param algorithm: A case-insensitive string, specifying which\n",
      "     |          algorithm should be used to train the classifier.  The\n",
      "     |          following algorithms are currently available.\n",
      "     |      \n",
      "     |          - Iterative Scaling Methods: Generalized Iterative Scaling (``'GIS'``),\n",
      "     |            Improved Iterative Scaling (``'IIS'``)\n",
      "     |          - External Libraries (requiring megam):\n",
      "     |            LM-BFGS algorithm, with training performed by Megam (``'megam'``)\n",
      "     |      \n",
      "     |          The default algorithm is ``'IIS'``.\n",
      "     |      \n",
      "     |      :type trace: int\n",
      "     |      :param trace: The level of diagnostic tracing output to produce.\n",
      "     |          Higher values produce more verbose output.\n",
      "     |      :type encoding: MaxentFeatureEncodingI\n",
      "     |      :param encoding: A feature encoding, used to convert featuresets\n",
      "     |          into feature vectors.  If none is specified, then a\n",
      "     |          ``BinaryMaxentFeatureEncoding`` will be built based on the\n",
      "     |          features that are attested in the training corpus.\n",
      "     |      :type labels: list(str)\n",
      "     |      :param labels: The set of possible labels.  If none is given, then\n",
      "     |          the set of all labels attested in the training data will be\n",
      "     |          used instead.\n",
      "     |      :param gaussian_prior_sigma: The sigma value for a gaussian\n",
      "     |          prior on model weights.  Currently, this is supported by\n",
      "     |          ``megam``. For other algorithms, its value is ignored.\n",
      "     |      :param cutoffs: Arguments specifying various conditions under\n",
      "     |          which the training should be halted.  (Some of the cutoff\n",
      "     |          conditions are not supported by some algorithms.)\n",
      "     |      \n",
      "     |          - ``max_iter=v``: Terminate after ``v`` iterations.\n",
      "     |          - ``min_ll=v``: Terminate after the negative average\n",
      "     |            log-likelihood drops under ``v``.\n",
      "     |          - ``min_lldelta=v``: Terminate if a single iteration improves\n",
      "     |            log likelihood by less than ``v``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from MaxentClassifier:\n",
      "     |  \n",
      "     |  __init__(self, encoding, weights, logarithmic=True)\n",
      "     |      Construct a new maxent classifier model.  Typically, new\n",
      "     |      classifier models are created using the ``train()`` method.\n",
      "     |      \n",
      "     |      :type encoding: MaxentFeatureEncodingI\n",
      "     |      :param encoding: An encoding that is used to convert the\n",
      "     |          featuresets that are given to the ``classify`` method into\n",
      "     |          joint-feature vectors, which are used by the maxent\n",
      "     |          classifier model.\n",
      "     |      \n",
      "     |      :type weights: list of float\n",
      "     |      :param weights:  The feature weight vector for this classifier.\n",
      "     |      \n",
      "     |      :type logarithmic: bool\n",
      "     |      :param logarithmic: If false, then use non-logarithmic weights.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |  \n",
      "     |  classify(self, featureset)\n",
      "     |      :return: the most appropriate label for the given featureset.\n",
      "     |      :rtype: label\n",
      "     |  \n",
      "     |  explain(self, featureset, columns=4)\n",
      "     |      Print a table showing the effect of each of the features in\n",
      "     |      the given feature set, and how they combine to determine the\n",
      "     |      probabilities of each label for that featureset.\n",
      "     |  \n",
      "     |  labels(self)\n",
      "     |      :return: the list of category labels used by this classifier.\n",
      "     |      :rtype: list of (immutable)\n",
      "     |  \n",
      "     |  most_informative_features(self, n=10)\n",
      "     |      Generates the ranked list of informative features from most to least.\n",
      "     |  \n",
      "     |  prob_classify(self, featureset)\n",
      "     |      :return: a probability distribution over labels for the given\n",
      "     |          featureset.\n",
      "     |      :rtype: ProbDistI\n",
      "     |  \n",
      "     |  set_weights(self, new_weights)\n",
      "     |      Set the feature weight vector for this classifier.\n",
      "     |      :param new_weights: The new feature weight vector.\n",
      "     |      :type new_weights: list of float\n",
      "     |  \n",
      "     |  show_most_informative_features(self, n=10, show='all')\n",
      "     |      :param show: all, neg, or pos (for negative-only or positive-only)\n",
      "     |      :type show: str\n",
      "     |      :param n: The no. of top features\n",
      "     |      :type n: int\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  weights(self)\n",
      "     |      :return: The feature weight vector for this classifier.\n",
      "     |      :rtype: list of float\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from MaxentClassifier:\n",
      "     |  \n",
      "     |  ALGORITHMS = ['GIS', 'IIS', 'MEGAM', 'TADM']\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.classify.api.ClassifierI:\n",
      "     |  \n",
      "     |  classify_many(self, featuresets)\n",
      "     |      Apply ``self.classify()`` to each element of ``featuresets``.  I.e.:\n",
      "     |      \n",
      "     |          return [self.classify(fs) for fs in featuresets]\n",
      "     |      \n",
      "     |      :rtype: list(label)\n",
      "     |  \n",
      "     |  prob_classify_many(self, featuresets)\n",
      "     |      Apply ``self.prob_classify()`` to each element of ``featuresets``.  I.e.:\n",
      "     |      \n",
      "     |          return [self.prob_classify(fs) for fs in featuresets]\n",
      "     |      \n",
      "     |      :rtype: list(ProbDistI)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.classify.api.ClassifierI:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TypedMaxentFeatureEncoding(MaxentFeatureEncodingI)\n",
      "     |  TypedMaxentFeatureEncoding(labels, mapping, unseen_features=False, alwayson_features=False)\n",
      "     |  \n",
      "     |  A feature encoding that generates vectors containing integer,\n",
      "     |  float and binary joint-features of the form:\n",
      "     |  \n",
      "     |  Binary (for string and boolean features):\n",
      "     |  \n",
      "     |  |  joint_feat(fs, l) = { 1 if (fs[fname] == fval) and (l == label)\n",
      "     |  |                      {\n",
      "     |  |                      { 0 otherwise\n",
      "     |  \n",
      "     |  Value (for integer and float features):\n",
      "     |  \n",
      "     |  |  joint_feat(fs, l) = { fval if     (fs[fname] == type(fval))\n",
      "     |  |                      {         and (l == label)\n",
      "     |  |                      {\n",
      "     |  |                      { not encoded otherwise\n",
      "     |  \n",
      "     |  Where ``fname`` is the name of an input-feature, ``fval`` is a value\n",
      "     |  for that input-feature, and ``label`` is a label.\n",
      "     |  \n",
      "     |  Typically, these features are constructed based on a training\n",
      "     |  corpus, using the ``train()`` method.\n",
      "     |  \n",
      "     |  For string and boolean features [type(fval) not in (int, float)]\n",
      "     |  this method will create one feature for each combination of\n",
      "     |  ``fname``, ``fval``, and ``label`` that occurs at least once in the\n",
      "     |  training corpus.\n",
      "     |  \n",
      "     |  For integer and float features [type(fval) in (int, float)] this\n",
      "     |  method will create one feature for each combination of ``fname``\n",
      "     |  and ``label`` that occurs at least once in the training corpus.\n",
      "     |  \n",
      "     |  For binary features the ``unseen_features`` parameter can be used\n",
      "     |  to add \"unseen-value features\", which are used whenever an input\n",
      "     |  feature has a value that was not encountered in the training\n",
      "     |  corpus.  These features have the form:\n",
      "     |  \n",
      "     |  |  joint_feat(fs, l) = { 1 if is_unseen(fname, fs[fname])\n",
      "     |  |                      {      and l == label\n",
      "     |  |                      {\n",
      "     |  |                      { 0 otherwise\n",
      "     |  \n",
      "     |  Where ``is_unseen(fname, fval)`` is true if the encoding does not\n",
      "     |  contain any joint features that are true when ``fs[fname]==fval``.\n",
      "     |  \n",
      "     |  The ``alwayson_features`` parameter can be used to add \"always-on\n",
      "     |  features\", which have the form:\n",
      "     |  \n",
      "     |  |  joint_feat(fs, l) = { 1 if (l == label)\n",
      "     |  |                      {\n",
      "     |  |                      { 0 otherwise\n",
      "     |  \n",
      "     |  These always-on features allow the maxent model to directly model\n",
      "     |  the prior probabilities of each label.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TypedMaxentFeatureEncoding\n",
      "     |      MaxentFeatureEncodingI\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, labels, mapping, unseen_features=False, alwayson_features=False)\n",
      "     |      :param labels: A list of the \"known labels\" for this encoding.\n",
      "     |      \n",
      "     |      :param mapping: A dictionary mapping from ``(fname,fval,label)``\n",
      "     |          tuples to corresponding joint-feature indexes.  These\n",
      "     |          indexes must be the set of integers from 0...len(mapping).\n",
      "     |          If ``mapping[fname,fval,label]=id``, then\n",
      "     |          ``self.encode({..., fname:fval, ...``, label)[id]} is 1;\n",
      "     |          otherwise, it is 0.\n",
      "     |      \n",
      "     |      :param unseen_features: If true, then include unseen value\n",
      "     |         features in the generated joint-feature vectors.\n",
      "     |      \n",
      "     |      :param alwayson_features: If true, then include always-on\n",
      "     |         features in the generated joint-feature vectors.\n",
      "     |  \n",
      "     |  describe(self, f_id)\n",
      "     |      :return: A string describing the value of the joint-feature\n",
      "     |          whose index in the generated feature vectors is ``fid``.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  encode(self, featureset, label)\n",
      "     |      Given a (featureset, label) pair, return the corresponding\n",
      "     |      vector of joint-feature values.  This vector is represented as\n",
      "     |      a list of ``(index, value)`` tuples, specifying the value of\n",
      "     |      each non-zero joint-feature.\n",
      "     |      \n",
      "     |      :type featureset: dict\n",
      "     |      :rtype: list(tuple(int, int))\n",
      "     |  \n",
      "     |  labels(self)\n",
      "     |      :return: A list of the \"known labels\" -- i.e., all labels\n",
      "     |          ``l`` such that ``self.encode(fs,l)`` can be a nonzero\n",
      "     |          joint-feature vector for some value of ``fs``.\n",
      "     |      :rtype: list\n",
      "     |  \n",
      "     |  length(self)\n",
      "     |      :return: The size of the fixed-length joint-feature vectors\n",
      "     |          that are generated by this encoding.\n",
      "     |      :rtype: int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  train(train_toks, count_cutoff=0, labels=None, **options) from builtins.type\n",
      "     |      Construct and return new feature encoding, based on a given\n",
      "     |      training corpus ``train_toks``.  See the class description\n",
      "     |      ``TypedMaxentFeatureEncoding`` for a description of the\n",
      "     |      joint-features that will be included in this encoding.\n",
      "     |      \n",
      "     |      Note: recognized feature values types are (int, float), over\n",
      "     |      types are interpreted as regular binary features.\n",
      "     |      \n",
      "     |      :type train_toks: list(tuple(dict, str))\n",
      "     |      :param train_toks: Training data, represented as a list of\n",
      "     |          pairs, the first member of which is a feature dictionary,\n",
      "     |          and the second of which is a classification label.\n",
      "     |      \n",
      "     |      :type count_cutoff: int\n",
      "     |      :param count_cutoff: A cutoff value that is used to discard\n",
      "     |          rare joint-features.  If a joint-feature's value is 1\n",
      "     |          fewer than ``count_cutoff`` times in the training corpus,\n",
      "     |          then that joint-feature is not included in the generated\n",
      "     |          encoding.\n",
      "     |      \n",
      "     |      :type labels: list\n",
      "     |      :param labels: A list of labels that should be used by the\n",
      "     |          classifier.  If not specified, then the set of labels\n",
      "     |          attested in ``train_toks`` will be used.\n",
      "     |      \n",
      "     |      :param options: Extra parameters for the constructor, such as\n",
      "     |          ``unseen_features`` and ``alwayson_features``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from MaxentFeatureEncodingI:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    calculate_deltas(train_toks, classifier, unattested, ffreq_empirical, nfmap, nfarray, nftranspose, encoding)\n",
      "        Calculate the update values for the classifier weights for\n",
      "        this iteration of IIS.  These update weights are the value of\n",
      "        ``delta`` that solves the equation::\n",
      "        \n",
      "          ffreq_empirical[i]\n",
      "                 =\n",
      "          SUM[fs,l] (classifier.prob_classify(fs).prob(l) *\n",
      "                     feature_vector(fs,l)[i] *\n",
      "                     exp(delta[i] * nf(feature_vector(fs,l))))\n",
      "        \n",
      "        Where:\n",
      "            - *(fs,l)* is a (featureset, label) tuple from ``train_toks``\n",
      "            - *feature_vector(fs,l)* = ``encoding.encode(fs,l)``\n",
      "            - *nf(vector)* = ``sum([val for (id,val) in vector])``\n",
      "        \n",
      "        This method uses Newton's method to solve this equation for\n",
      "        *delta[i]*.  In particular, it starts with a guess of\n",
      "        ``delta[i]`` = 1; and iteratively updates ``delta`` with:\n",
      "        \n",
      "        | delta[i] -= (ffreq_empirical[i] - sum1[i])/(-sum2[i])\n",
      "        \n",
      "        until convergence, where *sum1* and *sum2* are defined as:\n",
      "        \n",
      "        |    sum1[i](delta) = SUM[fs,l] f[i](fs,l,delta)\n",
      "        |    sum2[i](delta) = SUM[fs,l] (f[i](fs,l,delta).nf(feature_vector(fs,l)))\n",
      "        |    f[i](fs,l,delta) = (classifier.prob_classify(fs).prob(l) .\n",
      "        |                        feature_vector(fs,l)[i] .\n",
      "        |                        exp(delta[i] . nf(feature_vector(fs,l))))\n",
      "        \n",
      "        Note that *sum1* and *sum2* depend on ``delta``; so they need\n",
      "        to be re-computed each iteration.\n",
      "        \n",
      "        The variables ``nfmap``, ``nfarray``, and ``nftranspose`` are\n",
      "        used to generate a dense encoding for *nf(ltext)*.  This\n",
      "        allows ``_deltas`` to calculate *sum1* and *sum2* using\n",
      "        matrices, which yields a significant performance improvement.\n",
      "        \n",
      "        :param train_toks: The set of training tokens.\n",
      "        :type train_toks: list(tuple(dict, str))\n",
      "        :param classifier: The current classifier.\n",
      "        :type classifier: ClassifierI\n",
      "        :param ffreq_empirical: An array containing the empirical\n",
      "            frequency for each feature.  The *i*\\ th element of this\n",
      "            array is the empirical frequency for feature *i*.\n",
      "        :type ffreq_empirical: sequence of float\n",
      "        :param unattested: An array that is 1 for features that are\n",
      "            not attested in the training data; and 0 for features that\n",
      "            are attested.  In other words, ``unattested[i]==0`` iff\n",
      "            ``ffreq_empirical[i]==0``.\n",
      "        :type unattested: sequence of int\n",
      "        :param nfmap: A map that can be used to compress ``nf`` to a dense\n",
      "            vector.\n",
      "        :type nfmap: dict(int -> int)\n",
      "        :param nfarray: An array that can be used to uncompress ``nf``\n",
      "            from a dense vector.\n",
      "        :type nfarray: array(float)\n",
      "        :param nftranspose: The transpose of ``nfarray``\n",
      "        :type nftranspose: array(float)\n",
      "    \n",
      "    calculate_empirical_fcount(train_toks, encoding)\n",
      "    \n",
      "    calculate_estimated_fcount(classifier, train_toks, encoding)\n",
      "    \n",
      "    calculate_nfmap(train_toks, encoding)\n",
      "        Construct a map that can be used to compress ``nf`` (which is\n",
      "        typically sparse).\n",
      "        \n",
      "        *nf(feature_vector)* is the sum of the feature values for\n",
      "        *feature_vector*.\n",
      "        \n",
      "        This represents the number of features that are active for a\n",
      "        given labeled text.  This method finds all values of *nf(t)*\n",
      "        that are attested for at least one token in the given list of\n",
      "        training tokens; and constructs a dictionary mapping these\n",
      "        attested values to a continuous range *0...N*.  For example,\n",
      "        if the only values of *nf()* that were attested were 3, 5, and\n",
      "        7, then ``_nfmap`` might return the dictionary ``{3:0, 5:1, 7:2}``.\n",
      "        \n",
      "        :return: A map that can be used to compress ``nf`` to a dense\n",
      "            vector.\n",
      "        :rtype: dict(int -> int)\n",
      "    \n",
      "    demo()\n",
      "        ######################################################################\n",
      "        # { Demo\n",
      "        ######################################################################\n",
      "    \n",
      "    train_maxent_classifier_with_gis(train_toks, trace=3, encoding=None, labels=None, **cutoffs)\n",
      "        Train a new ``ConditionalExponentialClassifier``, using the given\n",
      "        training samples, using the Generalized Iterative Scaling\n",
      "        algorithm.  This ``ConditionalExponentialClassifier`` will encode\n",
      "        the model that maximizes entropy from all the models that are\n",
      "        empirically consistent with ``train_toks``.\n",
      "        \n",
      "        :see: ``train_maxent_classifier()`` for parameter descriptions.\n",
      "    \n",
      "    train_maxent_classifier_with_iis(train_toks, trace=3, encoding=None, labels=None, **cutoffs)\n",
      "        Train a new ``ConditionalExponentialClassifier``, using the given\n",
      "        training samples, using the Improved Iterative Scaling algorithm.\n",
      "        This ``ConditionalExponentialClassifier`` will encode the model\n",
      "        that maximizes entropy from all the models that are empirically\n",
      "        consistent with ``train_toks``.\n",
      "        \n",
      "        :see: ``train_maxent_classifier()`` for parameter descriptions.\n",
      "    \n",
      "    train_maxent_classifier_with_megam(train_toks, trace=3, encoding=None, labels=None, gaussian_prior_sigma=0, **kwargs)\n",
      "        Train a new ``ConditionalExponentialClassifier``, using the given\n",
      "        training samples, using the external ``megam`` library.  This\n",
      "        ``ConditionalExponentialClassifier`` will encode the model that\n",
      "        maximizes entropy from all the models that are empirically\n",
      "        consistent with ``train_toks``.\n",
      "        \n",
      "        :see: ``train_maxent_classifier()`` for parameter descriptions.\n",
      "        :see: ``nltk.classify.megam``\n",
      "\n",
      "DATA\n",
      "    __docformat__ = 'epytext en'\n",
      "    integer_types = (<class 'int'>,)\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\junglebook\\anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.classify.maxent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxent = nltk.classify.maxent.MaxentClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.370\n",
      "             2          -0.37335        0.763\n",
      "             3          -0.37294        0.763\n",
      "             4          -0.37270        0.763\n",
      "             5          -0.37254        0.763\n",
      "             6          -0.37242        0.763\n",
      "      Training stopped: keyboard interrupt\n",
      "         Final          -0.37233        0.763\n"
     ]
    }
   ],
   "source": [
    "classifier = maxent.train(train_toks = train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33, 42, 34, 48, 46, 47, 35, 23, 24, 36]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.most_informative_features()\n",
    "# A bit weird "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'last_letter': 'h'}, 'female')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lo'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"Hello\"\n",
    "name[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.370\n",
      "             2          -0.34240        0.804\n",
      "             3          -0.33219        0.804\n",
      "             4          -0.32611        0.804\n",
      "             5          -0.32208        0.804\n",
      "             6          -0.31921        0.804\n",
      "             7          -0.31706        0.804\n",
      "      Training stopped: keyboard interrupt\n",
      "         Final          -0.31706        0.804\n"
     ]
    }
   ],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-2:]}\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = maxent.train(train_toks = train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[273, 428, 338, 311, 385, 176, 395, 212, 398, 307]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.370\n",
      "             2          -0.41089        0.786\n",
      "             3          -0.36031        0.812\n",
      "             4          -0.33878        0.813\n",
      "             5          -0.32729        0.814\n",
      "             6          -0.32028        0.813\n",
      "             7          -0.31557        0.813\n",
      "             8          -0.31217        0.814\n",
      "             9          -0.30957        0.814\n",
      "            10          -0.30751        0.814\n",
      "            11          -0.30581        0.814\n",
      "      Training stopped: keyboard interrupt\n",
      "         Final          -0.30581        0.814\n"
     ]
    }
   ],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-2:], \"first_letter\": word[0]}\n",
    "\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = maxent.train(train_toks = train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the last two letters increase performance quickly and adding the first letter as a feature might just account for idiosyncracies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. \n",
    "\n",
    "The Senseval 2 Corpus contains data intended to train word-sense disambiguation classifiers. It contains data for four words: hard, interest, line, and serve. Choose one of these four words, and load the corresponding data:\n",
    "\n",
    "Using this dataset, build a classifier that predicts the correct sense tag for a given instance. See the corpus HOWTO at http://nltk.org/howto for information on using the instance objects returned by the Senseval 2 Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import senseval\n",
    "instances = senseval.instances('hard.pos')\n",
    "size = int(len(instances) * 0.1)\n",
    "train_set, test_set = instances[size:], instances[:size]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
